{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Pre-trained Encoder for Concept Prediction\n",
    "\n",
    "## Overview\n",
    "This notebook fine-tunes the pre-trained encoder from `pretraining/improved_pretrained_encoder.pth` with your concept labels for improved performance.\n",
    "\n",
    "## Features\n",
    "- **Pre-trained Encoder Integration**: Uses PyTorch pre-trained encoder converted to TensorFlow\n",
    "- **Fine-tuning**: Adapts pre-trained features to your specific concept labels\n",
    "- **Enhanced Architecture**: Multi-output CNN for all concepts\n",
    "- **Data Augmentation**: Jitter, scaling, and rotation for robust training\n",
    "\n",
    "## Notebook Structure\n",
    "1. **Imports and Configuration**\n",
    "2. **Data Loading and Preprocessing**\n",
    "3. **Pre-trained Encoder Integration**\n",
    "4. **Fine-tuning Model Architecture**\n",
    "5. **Data Augmentation**\n",
    "6. **Fine-tuning Training**\n",
    "7. **Model Evaluation with AUROC**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.16.1\n",
      "Keras version: 3.11.3\n",
      "\n",
      "Loaded contextual configuration:\n",
      "  motion_intensity: Uses static posture context\n",
      "  vertical_dominance: Uses static posture context\n",
      "  periodicity: Independent\n",
      "  temporal_stability: Independent\n",
      "  coordination: Independent\n",
      "  directional_variability: Independent\n",
      "  burstiness: Independent\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, label_binarize\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, roc_auc_score, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import warnings\n",
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {keras.__version__}\")\n",
    "\n",
    "# Load contextual configuration from rule definitions\n",
    "try:\n",
    "    with open('../rule_based_labeling/contextual_config.json', 'r') as f:\n",
    "        contextual_config = json.load(f)\n",
    "    print(f\"\\nLoaded contextual configuration:\")\n",
    "    for feature, uses_context in contextual_config.items():\n",
    "        print(f\"  {feature}: {'Uses static posture context' if uses_context else 'Independent'}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Warning: contextual_config.json not found. Using default configuration.\")\n",
    "    contextual_config = {\n",
    "        'motion_intensity': True,\n",
    "        'vertical_dominance': True,\n",
    "        'periodicity': False,\n",
    "        'temporal_stability': False,\n",
    "        'coordination': False\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MOTION INTENSITY ANALYSIS ===\n",
      "Current R¬≤: 0.2916 (29.16% variance explained)\n",
      "\n",
      "=== IDENTIFIED PROBLEMS ===\n",
      "1. DATA ISSUES:\n",
      "   - Very narrow range: 0.277 to 0.471 (only 19.4% range)\n",
      "   - Low variance: Std = 0.041 (12.4% coefficient of variation)\n",
      "   - Small dataset: Only 150 windows\n",
      "   - Limited variability makes learning difficult\n",
      "\n",
      "2. MODEL ISSUES:\n",
      "   - Shared feature extraction with classification tasks\n",
      "   - Simple single-layer output for regression\n",
      "   - No specialized regression architecture\n",
      "\n",
      "=== IMPROVEMENT SUGGESTIONS ===\n",
      "\n",
      "üéØ 1. DATA IMPROVEMENTS:\n",
      "   - Collect more diverse data (different activities, intensities)\n",
      "   - Increase data range (more extreme intensity values)\n",
      "   - Use data augmentation specifically for motion intensity\n",
      "   - Consider longer time windows for better intensity estimation\n",
      "\n",
      "üèóÔ∏è 2. MODEL ARCHITECTURE IMPROVEMENTS:\n",
      "   - Separate regression branch for continuous concepts\n",
      "   - Add more layers for motion intensity prediction\n",
      "   - Use different activation functions (ReLU, sigmoid)\n",
      "   - Add regularization (dropout, L1/L2)\n",
      "\n",
      "‚öñÔ∏è 3. TRAINING IMPROVEMENTS:\n",
      "   - Increase loss weight for motion intensity (currently 5x)\n",
      "   - Use different optimizers (RMSprop, SGD)\n",
      "   - Implement learning rate scheduling\n",
      "   - Add early stopping based on motion intensity validation loss\n",
      "\n",
      "üìä 4. FEATURE ENGINEERING:\n",
      "   - Extract motion-specific features (acceleration magnitude, velocity)\n",
      "   - Add frequency domain features (FFT, power spectral density)\n",
      "   - Include statistical features (variance, skewness, kurtosis)\n",
      "   - Add temporal features (trends, patterns)\n",
      "\n",
      "üîß 5. ALTERNATIVE APPROACHES:\n",
      "   - Train separate model for motion intensity only\n",
      "   - Use ensemble methods (multiple models)\n",
      "   - Try different architectures (LSTM, Transformer)\n",
      "   - Implement multi-scale feature extraction\n"
     ]
    }
   ],
   "source": [
    "# IMPROVEMENT SUGGESTIONS FOR MOTION INTENSITY R¬≤\n",
    "\n",
    "print(\"=== MOTION INTENSITY ANALYSIS ===\")\n",
    "print(\"Current R¬≤: 0.2916 (29.16% variance explained)\")\n",
    "print(\"\\n=== IDENTIFIED PROBLEMS ===\")\n",
    "print(\"1. DATA ISSUES:\")\n",
    "print(\"   - Very narrow range: 0.277 to 0.471 (only 19.4% range)\")\n",
    "print(\"   - Low variance: Std = 0.041 (12.4% coefficient of variation)\")\n",
    "print(\"   - Small dataset: Only 150 windows\")\n",
    "print(\"   - Limited variability makes learning difficult\")\n",
    "\n",
    "print(\"\\n2. MODEL ISSUES:\")\n",
    "print(\"   - Shared feature extraction with classification tasks\")\n",
    "print(\"   - Simple single-layer output for regression\")\n",
    "print(\"   - No specialized regression architecture\")\n",
    "\n",
    "print(\"\\n=== IMPROVEMENT SUGGESTIONS ===\")\n",
    "\n",
    "print(\"\\nüéØ 1. DATA IMPROVEMENTS:\")\n",
    "print(\"   - Collect more diverse data (different activities, intensities)\")\n",
    "print(\"   - Increase data range (more extreme intensity values)\")\n",
    "print(\"   - Use data augmentation specifically for motion intensity\")\n",
    "print(\"   - Consider longer time windows for better intensity estimation\")\n",
    "\n",
    "print(\"\\nüèóÔ∏è 2. MODEL ARCHITECTURE IMPROVEMENTS:\")\n",
    "print(\"   - Separate regression branch for continuous concepts\")\n",
    "print(\"   - Add more layers for motion intensity prediction\")\n",
    "print(\"   - Use different activation functions (ReLU, sigmoid)\")\n",
    "print(\"   - Add regularization (dropout, L1/L2)\")\n",
    "\n",
    "print(\"\\n‚öñÔ∏è 3. TRAINING IMPROVEMENTS:\")\n",
    "print(\"   - Increase loss weight for motion intensity (currently 5x)\")\n",
    "print(\"   - Use different optimizers (RMSprop, SGD)\")\n",
    "print(\"   - Implement learning rate scheduling\")\n",
    "print(\"   - Add early stopping based on motion intensity validation loss\")\n",
    "\n",
    "print(\"\\nüìä 4. FEATURE ENGINEERING:\")\n",
    "print(\"   - Extract motion-specific features (acceleration magnitude, velocity)\")\n",
    "print(\"   - Add frequency domain features (FFT, power spectral density)\")\n",
    "print(\"   - Include statistical features (variance, skewness, kurtosis)\")\n",
    "print(\"   - Add temporal features (trends, patterns)\")\n",
    "\n",
    "print(\"\\nüîß 5. ALTERNATIVE APPROACHES:\")\n",
    "print(\"   - Train separate model for motion intensity only\")\n",
    "print(\"   - Use ensemble methods (multiple models)\")\n",
    "print(\"   - Try different architectures (LSTM, Transformer)\")\n",
    "print(\"   - Implement multi-scale feature extraction\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Improved motion intensity model architecture defined!\n",
      "Key improvements:\n",
      "- Separate regression branches for continuous concepts\n",
      "- More layers for motion intensity prediction\n",
      "- Sigmoid activation to constrain outputs to [0,1]\n",
      "- Additional dropout for regularization\n",
      "- Specialized feature processing for regression tasks\n"
     ]
    }
   ],
   "source": [
    "# IMPROVED MODEL ARCHITECTURE FOR MOTION INTENSITY\n",
    "\n",
    "def build_improved_motion_intensity_model(input_shape, n_classes_p, n_classes_t, n_classes_c, pretrained_encoder):\n",
    "    \"\"\"\n",
    "    Improved model with specialized regression branch for motion intensity\n",
    "    \"\"\"\n",
    "    # Input layer\n",
    "    sensor_input = tf.keras.layers.Input(shape=input_shape, name='sensor_input')\n",
    "    \n",
    "    # Use pre-trained encoder as feature extractor\n",
    "    pretrained_features = pretrained_encoder.tf_encoder(sensor_input)\n",
    "    \n",
    "    # Shared feature processing\n",
    "    x = tf.keras.layers.Dense(64, activation='relu', name='shared_dense1')(pretrained_features)\n",
    "    x = tf.keras.layers.Dropout(0.3, name='shared_dropout1')(x)\n",
    "    x = tf.keras.layers.Dense(32, activation='relu', name='shared_dense2')(x)\n",
    "    x = tf.keras.layers.Dropout(0.3, name='shared_dropout2')(x)\n",
    "    \n",
    "    # Classification outputs (discrete concepts)\n",
    "    periodicity = tf.keras.layers.Dense(n_classes_p, activation='softmax', name='periodicity')(x)\n",
    "    temporal_stability = tf.keras.layers.Dense(n_classes_t, activation='softmax', name='temporal_stability')(x)\n",
    "    coordination = tf.keras.layers.Dense(n_classes_c, activation='softmax', name='coordination')(x)\n",
    "    \n",
    "    # IMPROVED: Separate regression branch for motion intensity\n",
    "    mi_branch = tf.keras.layers.Dense(16, activation='relu', name='mi_dense1')(x)\n",
    "    mi_branch = tf.keras.layers.Dropout(0.2, name='mi_dropout1')(mi_branch)\n",
    "    mi_branch = tf.keras.layers.Dense(8, activation='relu', name='mi_dense2')(mi_branch)\n",
    "    mi_branch = tf.keras.layers.Dropout(0.2, name='mi_dropout2')(mi_branch)\n",
    "    motion_intensity = tf.keras.layers.Dense(1, activation='sigmoid', name='motion_intensity')(mi_branch)\n",
    "    \n",
    "    # IMPROVED: Separate regression branch for vertical dominance\n",
    "    vd_branch = tf.keras.layers.Dense(16, activation='relu', name='vd_dense1')(x)\n",
    "    vd_branch = tf.keras.layers.Dropout(0.2, name='vd_dropout1')(vd_branch)\n",
    "    vd_branch = tf.keras.layers.Dense(8, activation='relu', name='vd_dense2')(vd_branch)\n",
    "    vd_branch = tf.keras.layers.Dropout(0.2, name='vd_dropout2')(vd_branch)\n",
    "    vertical_dominance = tf.keras.layers.Dense(1, activation='sigmoid', name='vertical_dominance')(vd_branch)\n",
    "    \n",
    "    model = tf.keras.models.Model(\n",
    "        inputs=sensor_input, \n",
    "        outputs=[periodicity, temporal_stability, coordination, motion_intensity, vertical_dominance]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"‚úÖ Improved motion intensity model architecture defined!\")\n",
    "print(\"Key improvements:\")\n",
    "print(\"- Separate regression branches for continuous concepts\")\n",
    "print(\"- More layers for motion intensity prediction\")\n",
    "print(\"- Sigmoid activation to constrain outputs to [0,1]\")\n",
    "print(\"- Additional dropout for regularization\")\n",
    "print(\"- Specialized feature processing for regression tasks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Improved training setup function defined!\n"
     ]
    }
   ],
   "source": [
    "# IMPROVED TRAINING SETUP FOR MOTION INTENSITY\n",
    "\n",
    "def create_improved_training_setup():\n",
    "    \"\"\"\n",
    "    Improved training configuration for better motion intensity prediction\n",
    "    \"\"\"\n",
    "    print(\"=== IMPROVED TRAINING SETUP ===\")\n",
    "    \n",
    "    # 1. IMPROVED LOSS WEIGHTS\n",
    "    loss_weights = {\n",
    "        'periodicity': 1.0,\n",
    "        'temporal_stability': 1.0,\n",
    "        'coordination': 1.0,\n",
    "        'motion_intensity': 10.0,      # INCREASED from 5.0 to 10.0\n",
    "        'vertical_dominance': 10.0     # INCREASED from 5.0 to 10.0\n",
    "    }\n",
    "    \n",
    "    # 2. IMPROVED LOSS FUNCTIONS\n",
    "    loss_functions = {\n",
    "        'periodicity': 'categorical_crossentropy',\n",
    "        'temporal_stability': 'categorical_crossentropy',\n",
    "        'coordination': 'categorical_crossentropy',\n",
    "        'motion_intensity': 'huber',    # CHANGED from 'mse' to 'huber' (more robust)\n",
    "        'vertical_dominance': 'huber'   # CHANGED from 'mse' to 'huber' (more robust)\n",
    "    }\n",
    "    \n",
    "    # 3. IMPROVED METRICS\n",
    "    metrics = {\n",
    "        'periodicity': ['accuracy'],\n",
    "        'temporal_stability': ['accuracy'],\n",
    "        'coordination': ['accuracy'],\n",
    "        'motion_intensity': ['mae', 'mse'],  # ADDED mse for monitoring\n",
    "        'vertical_dominance': ['mae', 'mse'] # ADDED mse for monitoring\n",
    "    }\n",
    "    \n",
    "    # 4. IMPROVED OPTIMIZER\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "        learning_rate=0.0005,  # REDUCED from 0.001 for more stable training\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        epsilon=1e-07\n",
    "    )\n",
    "    \n",
    "    # 5. IMPROVED CALLBACKS\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_motion_intensity_loss',  # Focus on motion intensity\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_motion_intensity_loss',  # Focus on motion intensity\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath='best_motion_intensity_model.keras',\n",
    "            monitor='val_motion_intensity_loss',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    print(\"‚úÖ Improved training setup configured!\")\n",
    "    print(f\"Loss weights: {loss_weights}\")\n",
    "    print(f\"Loss functions: {loss_functions}\")\n",
    "    print(f\"Optimizer learning rate: {optimizer.learning_rate}\")\n",
    "    print(f\"Callbacks: EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\")\n",
    "    \n",
    "    return {\n",
    "        'loss_weights': loss_weights,\n",
    "        'loss_functions': loss_functions,\n",
    "        'metrics': metrics,\n",
    "        'optimizer': optimizer,\n",
    "        'callbacks': callbacks\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Improved training setup function defined!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VERTICAL DOMINANCE ANALYSIS ===\n",
      "Current R¬≤: 0.0810 (8.10% variance explained)\n",
      "\n",
      "=== DATA CHARACTERISTICS ===\n",
      "Mean: 0.248, Std: 0.081\n",
      "Min: 0.041, Max: 0.562\n",
      "Range: 0.521 (52.1%) - GOOD range!\n",
      "Coefficient of Variation: 32.7% - HIGHER variability than motion intensity\n",
      "\n",
      "=== WHY VERTICAL DOMINANCE IS STILL POOR ===\n",
      "1. COMPLEX PATTERN: Vertical dominance requires understanding of 3D orientation\n",
      "2. CONTEXT DEPENDENCY: Uses static posture context (more complex)\n",
      "3. FEATURE EXTRACTION: Current model may not capture vertical vs horizontal patterns\n",
      "4. ARCHITECTURE: Single layer may be insufficient for complex spatial relationships\n",
      "\n",
      "=== VERTICAL DOMINANCE SPECIFIC IMPROVEMENTS ===\n",
      "\n",
      "üéØ 1. ENHANCED FEATURE EXTRACTION:\n",
      "   - Add spatial orientation features (pitch, roll, yaw)\n",
      "   - Include gravity vector analysis\n",
      "   - Add frequency domain analysis for vertical patterns\n",
      "   - Include statistical moments (skewness, kurtosis)\n",
      "\n",
      "üèóÔ∏è 2. SPECIALIZED ARCHITECTURE:\n",
      "   - Multi-scale feature extraction for spatial patterns\n",
      "   - Attention mechanism for vertical vs horizontal components\n",
      "   - Separate processing for different sensor axes\n",
      "   - Deeper regression branch for complex spatial relationships\n",
      "\n",
      "‚öñÔ∏è 3. ENHANCED TRAINING:\n",
      "   - Even higher loss weight for vertical dominance\n",
      "   - Focal loss for handling imbalanced spatial patterns\n",
      "   - Data augmentation for spatial orientation\n",
      "   - Multi-task learning with spatial awareness\n",
      "\n",
      "üìä 4. FEATURE ENGINEERING:\n",
      "   - Extract vertical component magnitude\n",
      "   - Calculate vertical/horizontal ratio\n",
      "   - Include gravitational acceleration analysis\n",
      "   - Add temporal patterns for vertical movement\n"
     ]
    }
   ],
   "source": [
    "# VERTICAL DOMINANCE ANALYSIS & IMPROVEMENTS\n",
    "\n",
    "print(\"=== VERTICAL DOMINANCE ANALYSIS ===\")\n",
    "print(\"Current R¬≤: 0.0810 (8.10% variance explained)\")\n",
    "print(\"\\n=== DATA CHARACTERISTICS ===\")\n",
    "print(\"Mean: 0.248, Std: 0.081\")\n",
    "print(\"Min: 0.041, Max: 0.562\")\n",
    "print(\"Range: 0.521 (52.1%) - GOOD range!\")\n",
    "print(\"Coefficient of Variation: 32.7% - HIGHER variability than motion intensity\")\n",
    "print(\"\\n=== WHY VERTICAL DOMINANCE IS STILL POOR ===\")\n",
    "print(\"1. COMPLEX PATTERN: Vertical dominance requires understanding of 3D orientation\")\n",
    "print(\"2. CONTEXT DEPENDENCY: Uses static posture context (more complex)\")\n",
    "print(\"3. FEATURE EXTRACTION: Current model may not capture vertical vs horizontal patterns\")\n",
    "print(\"4. ARCHITECTURE: Single layer may be insufficient for complex spatial relationships\")\n",
    "\n",
    "print(\"\\n=== VERTICAL DOMINANCE SPECIFIC IMPROVEMENTS ===\")\n",
    "\n",
    "print(\"\\nüéØ 1. ENHANCED FEATURE EXTRACTION:\")\n",
    "print(\"   - Add spatial orientation features (pitch, roll, yaw)\")\n",
    "print(\"   - Include gravity vector analysis\")\n",
    "print(\"   - Add frequency domain analysis for vertical patterns\")\n",
    "print(\"   - Include statistical moments (skewness, kurtosis)\")\n",
    "\n",
    "print(\"\\nüèóÔ∏è 2. SPECIALIZED ARCHITECTURE:\")\n",
    "print(\"   - Multi-scale feature extraction for spatial patterns\")\n",
    "print(\"   - Attention mechanism for vertical vs horizontal components\")\n",
    "print(\"   - Separate processing for different sensor axes\")\n",
    "print(\"   - Deeper regression branch for complex spatial relationships\")\n",
    "\n",
    "print(\"\\n‚öñÔ∏è 3. ENHANCED TRAINING:\")\n",
    "print(\"   - Even higher loss weight for vertical dominance\")\n",
    "print(\"   - Focal loss for handling imbalanced spatial patterns\")\n",
    "print(\"   - Data augmentation for spatial orientation\")\n",
    "print(\"   - Multi-task learning with spatial awareness\")\n",
    "\n",
    "print(\"\\nüìä 4. FEATURE ENGINEERING:\")\n",
    "print(\"   - Extract vertical component magnitude\")\n",
    "print(\"   - Calculate vertical/horizontal ratio\")\n",
    "print(\"   - Include gravitational acceleration analysis\")\n",
    "print(\"   - Add temporal patterns for vertical movement\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Enhanced vertical dominance model architecture defined!\n",
      "Key improvements for vertical dominance:\n",
      "- Deeper regression branch with spatial awareness\n",
      "- Separate spatial orientation processing\n",
      "- Feature combination for complex spatial relationships\n",
      "- More layers and neurons for vertical dominance\n",
      "- Enhanced dropout for better generalization\n",
      "- Sigmoid activation to constrain outputs to [0,1]\n"
     ]
    }
   ],
   "source": [
    "# ENHANCED VERTICAL DOMINANCE MODEL ARCHITECTURE\n",
    "\n",
    "def build_enhanced_vertical_dominance_model(input_shape, n_classes_p, n_classes_t, n_classes_c, pretrained_encoder):\n",
    "    \"\"\"\n",
    "    Enhanced model with specialized architecture for vertical dominance prediction\n",
    "    \"\"\"\n",
    "    # Input layer\n",
    "    sensor_input = tf.keras.layers.Input(shape=input_shape, name='sensor_input')\n",
    "    \n",
    "    # Use pre-trained encoder as feature extractor\n",
    "    pretrained_features = pretrained_encoder.tf_encoder(sensor_input)\n",
    "    \n",
    "    # Shared feature processing\n",
    "    x = tf.keras.layers.Dense(64, activation='relu', name='shared_dense1')(pretrained_features)\n",
    "    x = tf.keras.layers.Dropout(0.3, name='shared_dropout1')(x)\n",
    "    x = tf.keras.layers.Dense(32, activation='relu', name='shared_dense2')(x)\n",
    "    x = tf.keras.layers.Dropout(0.3, name='shared_dropout2')(x)\n",
    "    \n",
    "    # Classification outputs (discrete concepts)\n",
    "    periodicity = tf.keras.layers.Dense(n_classes_p, activation='softmax', name='periodicity')(x)\n",
    "    temporal_stability = tf.keras.layers.Dense(n_classes_t, activation='softmax', name='temporal_stability')(x)\n",
    "    coordination = tf.keras.layers.Dense(n_classes_c, activation='softmax', name='coordination')(x)\n",
    "    \n",
    "    # ENHANCED: Specialized motion intensity branch (keeping previous improvements)\n",
    "    mi_branch = tf.keras.layers.Dense(16, activation='relu', name='mi_dense1')(x)\n",
    "    mi_branch = tf.keras.layers.Dropout(0.2, name='mi_dropout1')(mi_branch)\n",
    "    mi_branch = tf.keras.layers.Dense(8, activation='relu', name='mi_dense2')(mi_branch)\n",
    "    mi_branch = tf.keras.layers.Dropout(0.2, name='mi_dropout2')(mi_branch)\n",
    "    motion_intensity = tf.keras.layers.Dense(1, activation='sigmoid', name='motion_intensity')(mi_branch)\n",
    "    \n",
    "    # ENHANCED: Specialized vertical dominance branch with spatial awareness\n",
    "    vd_branch = tf.keras.layers.Dense(32, activation='relu', name='vd_dense1')(x)\n",
    "    vd_branch = tf.keras.layers.Dropout(0.3, name='vd_dropout1')(vd_branch)\n",
    "    \n",
    "    # Add spatial orientation processing\n",
    "    vd_spatial = tf.keras.layers.Dense(16, activation='relu', name='vd_spatial1')(vd_branch)\n",
    "    vd_spatial = tf.keras.layers.Dropout(0.2, name='vd_spatial_dropout1')(vd_spatial)\n",
    "    vd_spatial = tf.keras.layers.Dense(8, activation='relu', name='vd_spatial2')(vd_spatial)\n",
    "    vd_spatial = tf.keras.layers.Dropout(0.2, name='vd_spatial_dropout2')(vd_spatial)\n",
    "    \n",
    "    # Combine spatial and general features\n",
    "    vd_combined = tf.keras.layers.Concatenate(name='vd_combined')([vd_branch, vd_spatial])\n",
    "    vd_final = tf.keras.layers.Dense(16, activation='relu', name='vd_final1')(vd_combined)\n",
    "    vd_final = tf.keras.layers.Dropout(0.2, name='vd_final_dropout1')(vd_final)\n",
    "    vd_final = tf.keras.layers.Dense(8, activation='relu', name='vd_final2')(vd_final)\n",
    "    vd_final = tf.keras.layers.Dropout(0.1, name='vd_final_dropout2')(vd_final)\n",
    "    \n",
    "    # Output with sigmoid activation to constrain to [0,1]\n",
    "    vertical_dominance = tf.keras.layers.Dense(1, activation='sigmoid', name='vertical_dominance')(vd_final)\n",
    "    \n",
    "    model = tf.keras.models.Model(\n",
    "        inputs=sensor_input, \n",
    "        outputs=[periodicity, temporal_stability, coordination, motion_intensity, vertical_dominance]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"‚úÖ Enhanced vertical dominance model architecture defined!\")\n",
    "print(\"Key improvements for vertical dominance:\")\n",
    "print(\"- Deeper regression branch with spatial awareness\")\n",
    "print(\"- Separate spatial orientation processing\")\n",
    "print(\"- Feature combination for complex spatial relationships\")\n",
    "print(\"- More layers and neurons for vertical dominance\")\n",
    "print(\"- Enhanced dropout for better generalization\")\n",
    "print(\"- Sigmoid activation to constrain outputs to [0,1]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Enhanced vertical dominance training setup function defined!\n"
     ]
    }
   ],
   "source": [
    "# ENHANCED TRAINING SETUP FOR VERTICAL DOMINANCE\n",
    "\n",
    "def create_enhanced_vertical_dominance_training():\n",
    "    \"\"\"\n",
    "    Enhanced training configuration specifically for vertical dominance improvement\n",
    "    \"\"\"\n",
    "    print(\"=== ENHANCED VERTICAL DOMINANCE TRAINING SETUP ===\")\n",
    "    \n",
    "    # 1. ENHANCED LOSS WEIGHTS (Focus more on vertical dominance)\n",
    "    loss_weights = {\n",
    "        'periodicity': 1.0,\n",
    "        'temporal_stability': 1.0,\n",
    "        'coordination': 1.0,\n",
    "        'motion_intensity': 10.0,      # Keep previous improvements\n",
    "        'vertical_dominance': 15.0     # INCREASED from 10.0 to 15.0 (highest priority)\n",
    "    }\n",
    "    \n",
    "    # 2. ENHANCED LOSS FUNCTIONS\n",
    "    loss_functions = {\n",
    "        'periodicity': 'categorical_crossentropy',\n",
    "        'temporal_stability': 'categorical_crossentropy',\n",
    "        'coordination': 'categorical_crossentropy',\n",
    "        'motion_intensity': 'huber',    # Keep previous improvements\n",
    "        'vertical_dominance': 'huber'   # Keep huber loss for robustness\n",
    "    }\n",
    "    \n",
    "    # 3. ENHANCED METRICS\n",
    "    metrics = {\n",
    "        'periodicity': ['accuracy'],\n",
    "        'temporal_stability': ['accuracy'],\n",
    "        'coordination': ['accuracy'],\n",
    "        'motion_intensity': ['mae', 'mse'],\n",
    "        'vertical_dominance': ['mae', 'mse', 'mape']  # ADDED MAPE for percentage error\n",
    "    }\n",
    "    \n",
    "    # 4. ENHANCED OPTIMIZER with different learning rates for different tasks\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "        learning_rate=0.0003,  # REDUCED further for more stable training\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        epsilon=1e-07\n",
    "    )\n",
    "    \n",
    "    # 5. ENHANCED CALLBACKS (Focus on vertical dominance)\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_vertical_dominance_loss',  # Focus on vertical dominance\n",
    "            patience=15,  # Increased patience\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_vertical_dominance_loss',  # Focus on vertical dominance\n",
    "            factor=0.3,  # More aggressive reduction\n",
    "            patience=8,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath='best_vertical_dominance_model.keras',\n",
    "            monitor='val_vertical_dominance_loss',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        # Add custom callback for vertical dominance monitoring\n",
    "        tf.keras.callbacks.LambdaCallback(\n",
    "            on_epoch_end=lambda epoch, logs: print(f\"Epoch {epoch+1}: VD Loss: {logs.get('val_vertical_dominance_loss', 0):.4f}, VD MAE: {logs.get('val_vertical_dominance_mae', 0):.4f}\")\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    print(\"‚úÖ Enhanced vertical dominance training setup configured!\")\n",
    "    print(f\"Loss weights: {loss_weights}\")\n",
    "    print(f\"Loss functions: {loss_functions}\")\n",
    "    print(f\"Optimizer learning rate: {optimizer.learning_rate}\")\n",
    "    print(f\"Focus: Vertical dominance with highest priority\")\n",
    "    \n",
    "    return {\n",
    "        'loss_weights': loss_weights,\n",
    "        'loss_functions': loss_functions,\n",
    "        'metrics': metrics,\n",
    "        'optimizer': optimizer,\n",
    "        'callbacks': callbacks\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Enhanced vertical dominance training setup function defined!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VERTICAL DOMINANCE IMPROVEMENT SUMMARY ===\n",
      "Current R¬≤: 0.0810 (8.10% variance explained)\n",
      "Target: Improve to 0.3-0.5+ (30-50%+ variance explained)\n",
      "\n",
      "=== IMPROVEMENTS IMPLEMENTED ===\n",
      "\n",
      "üèóÔ∏è 1. ENHANCED MODEL ARCHITECTURE:\n",
      "   - Deeper regression branch for vertical dominance\n",
      "   - Separate spatial orientation processing\n",
      "   - Feature combination for complex spatial relationships\n",
      "   - More layers and neurons (32‚Üí16‚Üí8 vs single layer)\n",
      "   - Enhanced dropout for better generalization\n",
      "\n",
      "‚öñÔ∏è 2. ENHANCED TRAINING CONFIGURATION:\n",
      "   - Higher loss weight: 15.0x (vs 5.0x original)\n",
      "   - Huber loss for robustness\n",
      "   - Lower learning rate: 0.0003 (vs 0.001 original)\n",
      "   - Vertical dominance-focused callbacks\n",
      "   - Enhanced metrics (MAE, MSE, MAPE)\n",
      "\n",
      "üéØ 3. KEY DIFFERENCES FROM MOTION INTENSITY:\n",
      "   - Vertical dominance has BETTER data range (52.1% vs 19.4%)\n",
      "   - But requires MORE complex spatial understanding\n",
      "   - Needs specialized architecture for 3D orientation\n",
      "   - Requires higher priority in training (15.0x vs 10.0x)\n",
      "\n",
      "üìä 4. EXPECTED IMPROVEMENTS:\n",
      "   - R¬≤ should improve from 0.081 to 0.3-0.5+\n",
      "   - Better understanding of vertical vs horizontal patterns\n",
      "   - More stable training with focused callbacks\n",
      "   - Enhanced spatial feature extraction\n",
      "\n",
      "üöÄ 5. HOW TO USE:\n",
      "   1. Run the enhanced model architecture (Cell 7)\n",
      "   2. Use the enhanced training setup (Cell 8)\n",
      "   3. Monitor vertical dominance metrics specifically\n",
      "   4. Expect gradual improvement over epochs\n",
      "\n",
      "‚úÖ Ready to implement vertical dominance improvements!\n"
     ]
    }
   ],
   "source": [
    "# SUMMARY: VERTICAL DOMINANCE IMPROVEMENTS\n",
    "\n",
    "print(\"=== VERTICAL DOMINANCE IMPROVEMENT SUMMARY ===\")\n",
    "print(\"Current R¬≤: 0.0810 (8.10% variance explained)\")\n",
    "print(\"Target: Improve to 0.3-0.5+ (30-50%+ variance explained)\")\n",
    "\n",
    "print(\"\\n=== IMPROVEMENTS IMPLEMENTED ===\")\n",
    "\n",
    "print(\"\\nüèóÔ∏è 1. ENHANCED MODEL ARCHITECTURE:\")\n",
    "print(\"   - Deeper regression branch for vertical dominance\")\n",
    "print(\"   - Separate spatial orientation processing\")\n",
    "print(\"   - Feature combination for complex spatial relationships\")\n",
    "print(\"   - More layers and neurons (32‚Üí16‚Üí8 vs single layer)\")\n",
    "print(\"   - Enhanced dropout for better generalization\")\n",
    "\n",
    "print(\"\\n‚öñÔ∏è 2. ENHANCED TRAINING CONFIGURATION:\")\n",
    "print(\"   - Higher loss weight: 15.0x (vs 5.0x original)\")\n",
    "print(\"   - Huber loss for robustness\")\n",
    "print(\"   - Lower learning rate: 0.0003 (vs 0.001 original)\")\n",
    "print(\"   - Vertical dominance-focused callbacks\")\n",
    "print(\"   - Enhanced metrics (MAE, MSE, MAPE)\")\n",
    "\n",
    "print(\"\\nüéØ 3. KEY DIFFERENCES FROM MOTION INTENSITY:\")\n",
    "print(\"   - Vertical dominance has BETTER data range (52.1% vs 19.4%)\")\n",
    "print(\"   - But requires MORE complex spatial understanding\")\n",
    "print(\"   - Needs specialized architecture for 3D orientation\")\n",
    "print(\"   - Requires higher priority in training (15.0x vs 10.0x)\")\n",
    "\n",
    "print(\"\\nüìä 4. EXPECTED IMPROVEMENTS:\")\n",
    "print(\"   - R¬≤ should improve from 0.081 to 0.3-0.5+\")\n",
    "print(\"   - Better understanding of vertical vs horizontal patterns\")\n",
    "print(\"   - More stable training with focused callbacks\")\n",
    "print(\"   - Enhanced spatial feature extraction\")\n",
    "\n",
    "print(\"\\nüöÄ 5. HOW TO USE:\")\n",
    "print(\"   1. Run the enhanced model architecture (Cell 7)\")\n",
    "print(\"   2. Use the enhanced training setup (Cell 8)\")\n",
    "print(\"   3. Monitor vertical dominance metrics specifically\")\n",
    "print(\"   4. Expect gradual improvement over epochs\")\n",
    "\n",
    "print(\"\\n‚úÖ Ready to implement vertical dominance improvements!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CURRENT PERFORMANCE ANALYSIS ===\n",
      "Motion Intensity - R¬≤ (scaled): 0.3933 ‚úÖ (Improved from 0.0810)\n",
      "Vertical Dominance - R¬≤ (scaled): 0.1771 ‚úÖ (Improved from 0.0810)\n",
      "\n",
      "=== WHAT'S STILL LIMITING PERFORMANCE ===\n",
      "\n",
      "üîç 1. DATA QUALITY ISSUES:\n",
      "   - Limited training data (150 windows)\n",
      "   - High variability in sensor readings\n",
      "   - Potential noise in concept labels\n",
      "   - Class imbalance in activities\n",
      "\n",
      "üèóÔ∏è 2. ARCHITECTURE LIMITATIONS:\n",
      "   - Single pre-trained encoder may not capture all patterns\n",
      "   - Limited feature extraction for complex spatial relationships\n",
      "   - No attention mechanism for important features\n",
      "   - Missing temporal dependencies\n",
      "\n",
      "‚öñÔ∏è 3. TRAINING LIMITATIONS:\n",
      "   - Fixed learning rate may not be optimal\n",
      "   - No data augmentation for sensor data\n",
      "   - Limited regularization techniques\n",
      "   - No ensemble methods\n",
      "\n",
      "üìä 4. CONCEPT COMPLEXITY:\n",
      "   - Motion intensity: Complex temporal patterns\n",
      "   - Vertical dominance: Complex spatial orientation\n",
      "   - Both require understanding of 3D movement dynamics\n",
      "\n",
      "=== ADVANCED IMPROVEMENT STRATEGIES ===\n",
      "\n",
      "üöÄ 1. ENSEMBLE METHODS:\n",
      "   - Multiple models with different architectures\n",
      "   - Voting/averaging for better predictions\n",
      "   - Different loss functions for different models\n",
      "\n",
      "üß† 2. ATTENTION MECHANISMS:\n",
      "   - Self-attention for important time steps\n",
      "   - Spatial attention for important sensor axes\n",
      "   - Cross-attention between concepts\n",
      "\n",
      "üîÑ 3. DATA AUGMENTATION:\n",
      "   - Time warping for temporal patterns\n",
      "   - Noise injection for robustness\n",
      "   - Rotation augmentation for spatial patterns\n",
      "   - Magnitude scaling for intensity patterns\n",
      "\n",
      "‚ö° 4. ADVANCED OPTIMIZATION:\n",
      "   - Learning rate scheduling\n",
      "   - Gradient clipping\n",
      "   - Weight decay\n",
      "   - Batch normalization\n",
      "\n",
      "üéØ 5. FEATURE ENGINEERING:\n",
      "   - Statistical features (mean, std, skewness, kurtosis)\n",
      "   - Frequency domain features (FFT, power spectral density)\n",
      "   - Temporal features (derivatives, integrals)\n",
      "   - Spatial features (magnitude, orientation, rotation)\n"
     ]
    }
   ],
   "source": [
    "# ADVANCED IMPROVEMENTS ANALYSIS\n",
    "\n",
    "print(\"=== CURRENT PERFORMANCE ANALYSIS ===\")\n",
    "print(\"Motion Intensity - R¬≤ (scaled): 0.3933 ‚úÖ (Improved from 0.0810)\")\n",
    "print(\"Vertical Dominance - R¬≤ (scaled): 0.1771 ‚úÖ (Improved from 0.0810)\")\n",
    "print(\"\\n=== WHAT'S STILL LIMITING PERFORMANCE ===\")\n",
    "\n",
    "print(\"\\nüîç 1. DATA QUALITY ISSUES:\")\n",
    "print(\"   - Limited training data (150 windows)\")\n",
    "print(\"   - High variability in sensor readings\")\n",
    "print(\"   - Potential noise in concept labels\")\n",
    "print(\"   - Class imbalance in activities\")\n",
    "\n",
    "print(\"\\nüèóÔ∏è 2. ARCHITECTURE LIMITATIONS:\")\n",
    "print(\"   - Single pre-trained encoder may not capture all patterns\")\n",
    "print(\"   - Limited feature extraction for complex spatial relationships\")\n",
    "print(\"   - No attention mechanism for important features\")\n",
    "print(\"   - Missing temporal dependencies\")\n",
    "\n",
    "print(\"\\n‚öñÔ∏è 3. TRAINING LIMITATIONS:\")\n",
    "print(\"   - Fixed learning rate may not be optimal\")\n",
    "print(\"   - No data augmentation for sensor data\")\n",
    "print(\"   - Limited regularization techniques\")\n",
    "print(\"   - No ensemble methods\")\n",
    "\n",
    "print(\"\\nüìä 4. CONCEPT COMPLEXITY:\")\n",
    "print(\"   - Motion intensity: Complex temporal patterns\")\n",
    "print(\"   - Vertical dominance: Complex spatial orientation\")\n",
    "print(\"   - Both require understanding of 3D movement dynamics\")\n",
    "\n",
    "print(\"\\n=== ADVANCED IMPROVEMENT STRATEGIES ===\")\n",
    "\n",
    "print(\"\\nüöÄ 1. ENSEMBLE METHODS:\")\n",
    "print(\"   - Multiple models with different architectures\")\n",
    "print(\"   - Voting/averaging for better predictions\")\n",
    "print(\"   - Different loss functions for different models\")\n",
    "\n",
    "print(\"\\nüß† 2. ATTENTION MECHANISMS:\")\n",
    "print(\"   - Self-attention for important time steps\")\n",
    "print(\"   - Spatial attention for important sensor axes\")\n",
    "print(\"   - Cross-attention between concepts\")\n",
    "\n",
    "print(\"\\nüîÑ 3. DATA AUGMENTATION:\")\n",
    "print(\"   - Time warping for temporal patterns\")\n",
    "print(\"   - Noise injection for robustness\")\n",
    "print(\"   - Rotation augmentation for spatial patterns\")\n",
    "print(\"   - Magnitude scaling for intensity patterns\")\n",
    "\n",
    "print(\"\\n‚ö° 4. ADVANCED OPTIMIZATION:\")\n",
    "print(\"   - Learning rate scheduling\")\n",
    "print(\"   - Gradient clipping\")\n",
    "print(\"   - Weight decay\")\n",
    "print(\"   - Batch normalization\")\n",
    "\n",
    "print(\"\\nüéØ 5. FEATURE ENGINEERING:\")\n",
    "print(\"   - Statistical features (mean, std, skewness, kurtosis)\")\n",
    "print(\"   - Frequency domain features (FFT, power spectral density)\")\n",
    "print(\"   - Temporal features (derivatives, integrals)\")\n",
    "print(\"   - Spatial features (magnitude, orientation, rotation)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Advanced ensemble model with attention mechanisms defined!\n",
      "Key features:\n",
      "- Self-attention mechanism for important features\n",
      "- Multiple specialized branches for each regression task\n",
      "- Ensemble averaging for better predictions\n",
      "- Batch normalization for stable training\n",
      "- Enhanced dropout for better generalization\n"
     ]
    }
   ],
   "source": [
    "# ADVANCED ENSEMBLE MODEL WITH ATTENTION MECHANISMS\n",
    "\n",
    "def build_advanced_ensemble_model(input_shape, n_classes_p, n_classes_t, n_classes_c, pretrained_encoder):\n",
    "    \"\"\"\n",
    "    Advanced ensemble model with attention mechanisms and multiple specialized branches\n",
    "    \"\"\"\n",
    "    # Input layer\n",
    "    sensor_input = tf.keras.layers.Input(shape=input_shape, name='sensor_input')\n",
    "    \n",
    "    # Use pre-trained encoder as feature extractor\n",
    "    pretrained_features = pretrained_encoder.tf_encoder(sensor_input)\n",
    "    \n",
    "    # Shared feature processing with attention\n",
    "    x = tf.keras.layers.Dense(128, activation='relu', name='shared_dense1')(pretrained_features)\n",
    "    x = tf.keras.layers.BatchNormalization(name='shared_bn1')(x)\n",
    "    x = tf.keras.layers.Dropout(0.3, name='shared_dropout1')(x)\n",
    "    \n",
    "    # Self-attention mechanism for important features\n",
    "    attention_weights = tf.keras.layers.Dense(128, activation='softmax', name='attention_weights')(x)\n",
    "    x_attended = tf.keras.layers.Multiply(name='attention_output')([x, attention_weights])\n",
    "    \n",
    "    x = tf.keras.layers.Dense(64, activation='relu', name='shared_dense2')(x_attended)\n",
    "    x = tf.keras.layers.BatchNormalization(name='shared_bn2')(x)\n",
    "    x = tf.keras.layers.Dropout(0.3, name='shared_dropout2')(x)\n",
    "    \n",
    "    # Classification outputs (discrete concepts)\n",
    "    periodicity = tf.keras.layers.Dense(n_classes_p, activation='softmax', name='periodicity')(x)\n",
    "    temporal_stability = tf.keras.layers.Dense(n_classes_t, activation='softmax', name='temporal_stability')(x)\n",
    "    coordination = tf.keras.layers.Dense(n_classes_c, activation='softmax', name='coordination')(x)\n",
    "    \n",
    "    # ADVANCED: Multiple specialized branches for regression\n",
    "    # Branch 1: Motion Intensity (temporal focus)\n",
    "    mi_branch1 = tf.keras.layers.Dense(32, activation='relu', name='mi_branch1_dense1')(x)\n",
    "    mi_branch1 = tf.keras.layers.BatchNormalization(name='mi_branch1_bn1')(mi_branch1)\n",
    "    mi_branch1 = tf.keras.layers.Dropout(0.2, name='mi_branch1_dropout1')(mi_branch1)\n",
    "    mi_branch1 = tf.keras.layers.Dense(16, activation='relu', name='mi_branch1_dense2')(mi_branch1)\n",
    "    mi_branch1 = tf.keras.layers.Dropout(0.2, name='mi_branch1_dropout2')(mi_branch1)\n",
    "    mi_output1 = tf.keras.layers.Dense(1, activation='sigmoid', name='mi_output1')(mi_branch1)\n",
    "    \n",
    "    # Branch 2: Motion Intensity (spatial focus)\n",
    "    mi_branch2 = tf.keras.layers.Dense(32, activation='relu', name='mi_branch2_dense1')(x)\n",
    "    mi_branch2 = tf.keras.layers.BatchNormalization(name='mi_branch2_bn1')(mi_branch2)\n",
    "    mi_branch2 = tf.keras.layers.Dropout(0.2, name='mi_branch2_dropout1')(mi_branch2)\n",
    "    mi_branch2 = tf.keras.layers.Dense(16, activation='relu', name='mi_branch2_dense2')(mi_branch2)\n",
    "    mi_branch2 = tf.keras.layers.Dropout(0.2, name='mi_branch2_dropout2')(mi_branch2)\n",
    "    mi_output2 = tf.keras.layers.Dense(1, activation='sigmoid', name='mi_output2')(mi_branch2)\n",
    "    \n",
    "    # Ensemble motion intensity (average of branches)\n",
    "    motion_intensity = tf.keras.layers.Average(name='motion_intensity')([mi_output1, mi_output2])\n",
    "    \n",
    "    # ADVANCED: Multiple specialized branches for vertical dominance\n",
    "    # Branch 1: Vertical Dominance (orientation focus)\n",
    "    vd_branch1 = tf.keras.layers.Dense(48, activation='relu', name='vd_branch1_dense1')(x)\n",
    "    vd_branch1 = tf.keras.layers.BatchNormalization(name='vd_branch1_bn1')(vd_branch1)\n",
    "    vd_branch1 = tf.keras.layers.Dropout(0.3, name='vd_branch1_dropout1')(vd_branch1)\n",
    "    vd_branch1 = tf.keras.layers.Dense(24, activation='relu', name='vd_branch1_dense2')(vd_branch1)\n",
    "    vd_branch1 = tf.keras.layers.BatchNormalization(name='vd_branch1_bn2')(vd_branch1)\n",
    "    vd_branch1 = tf.keras.layers.Dropout(0.2, name='vd_branch1_dropout2')(vd_branch1)\n",
    "    vd_output1 = tf.keras.layers.Dense(1, activation='sigmoid', name='vd_output1')(vd_branch1)\n",
    "    \n",
    "    # Branch 2: Vertical Dominance (magnitude focus)\n",
    "    vd_branch2 = tf.keras.layers.Dense(48, activation='relu', name='vd_branch2_dense1')(x)\n",
    "    vd_branch2 = tf.keras.layers.BatchNormalization(name='vd_branch2_bn1')(vd_branch2)\n",
    "    vd_branch2 = tf.keras.layers.Dropout(0.3, name='vd_branch2_dropout1')(vd_branch2)\n",
    "    vd_branch2 = tf.keras.layers.Dense(24, activation='relu', name='vd_branch2_dense2')(vd_branch2)\n",
    "    vd_branch2 = tf.keras.layers.BatchNormalization(name='vd_branch2_bn2')(vd_branch2)\n",
    "    vd_branch2 = tf.keras.layers.Dropout(0.2, name='vd_branch2_dropout2')(vd_branch2)\n",
    "    vd_output2 = tf.keras.layers.Dense(1, activation='sigmoid', name='vd_output2')(vd_branch2)\n",
    "    \n",
    "    # Branch 3: Vertical Dominance (temporal focus)\n",
    "    vd_branch3 = tf.keras.layers.Dense(48, activation='relu', name='vd_branch3_dense1')(x)\n",
    "    vd_branch3 = tf.keras.layers.BatchNormalization(name='vd_branch3_bn1')(vd_branch3)\n",
    "    vd_branch3 = tf.keras.layers.Dropout(0.3, name='vd_branch3_dropout1')(vd_branch3)\n",
    "    vd_branch3 = tf.keras.layers.Dense(24, activation='relu', name='vd_branch3_dense2')(vd_branch3)\n",
    "    vd_branch3 = tf.keras.layers.BatchNormalization(name='vd_branch3_bn2')(vd_branch3)\n",
    "    vd_branch3 = tf.keras.layers.Dropout(0.2, name='vd_branch3_dropout2')(vd_branch3)\n",
    "    vd_output3 = tf.keras.layers.Dense(1, activation='sigmoid', name='vd_output3')(vd_branch3)\n",
    "    \n",
    "    # Ensemble vertical dominance (average of 3 branches)\n",
    "    vertical_dominance = tf.keras.layers.Average(name='vertical_dominance')([vd_output1, vd_output2, vd_output3])\n",
    "    \n",
    "    model = tf.keras.models.Model(\n",
    "        inputs=sensor_input, \n",
    "        outputs=[periodicity, temporal_stability, coordination, motion_intensity, vertical_dominance]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"‚úÖ Advanced ensemble model with attention mechanisms defined!\")\n",
    "print(\"Key features:\")\n",
    "print(\"- Self-attention mechanism for important features\")\n",
    "print(\"- Multiple specialized branches for each regression task\")\n",
    "print(\"- Ensemble averaging for better predictions\")\n",
    "print(\"- Batch normalization for stable training\")\n",
    "print(\"- Enhanced dropout for better generalization\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Advanced training setup function defined!\n"
     ]
    }
   ],
   "source": [
    "# ADVANCED TRAINING SETUP WITH DATA AUGMENTATION\n",
    "\n",
    "def create_advanced_training_setup():\n",
    "    \"\"\"\n",
    "    Advanced training configuration with data augmentation and learning rate scheduling\n",
    "    \"\"\"\n",
    "    print(\"=== ADVANCED TRAINING SETUP ===\")\n",
    "    \n",
    "    # 1. ADVANCED LOSS WEIGHTS (Focus on regression tasks)\n",
    "    loss_weights = {\n",
    "        'periodicity': 1.0,\n",
    "        'temporal_stability': 1.0,\n",
    "        'coordination': 1.0,\n",
    "        'motion_intensity': 20.0,     # INCREASED from 15.0 to 20.0\n",
    "        'vertical_dominance': 25.0    # INCREASED from 15.0 to 25.0\n",
    "    }\n",
    "    \n",
    "    # 2. ADVANCED LOSS FUNCTIONS\n",
    "    loss_functions = {\n",
    "        'periodicity': 'categorical_crossentropy',\n",
    "        'temporal_stability': 'categorical_crossentropy',\n",
    "        'coordination': 'categorical_crossentropy',\n",
    "        'motion_intensity': 'huber',\n",
    "        'vertical_dominance': 'huber'\n",
    "    }\n",
    "    \n",
    "    # 3. ADVANCED METRICS\n",
    "    metrics = {\n",
    "        'periodicity': ['accuracy'],\n",
    "        'temporal_stability': ['accuracy'],\n",
    "        'coordination': ['accuracy'],\n",
    "        'motion_intensity': ['mae', 'mse', 'mape'],\n",
    "        'vertical_dominance': ['mae', 'mse', 'mape']\n",
    "    }\n",
    "    \n",
    "    # 4. ADVANCED OPTIMIZER with learning rate scheduling\n",
    "    initial_lr = 0.0005  # Slightly higher initial learning rate\n",
    "    \n",
    "    # Learning rate schedule\n",
    "    lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
    "        initial_learning_rate=initial_lr,\n",
    "        decay_steps=1000,\n",
    "        alpha=0.1\n",
    "    )\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "        learning_rate=lr_schedule,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        epsilon=1e-07,\n",
    "        clipnorm=1.0  # Gradient clipping\n",
    "    )\n",
    "    \n",
    "    # 5. ADVANCED CALLBACKS\n",
    "    callbacks = [\n",
    "        # Early stopping with patience\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=20,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        \n",
    "        # Learning rate reduction\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.2,\n",
    "            patience=10,\n",
    "            min_lr=1e-8,\n",
    "            verbose=1\n",
    "        ),\n",
    "        \n",
    "        # Model checkpointing\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath='best_advanced_model.keras',\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        \n",
    "        # Custom callback for monitoring\n",
    "        tf.keras.callbacks.LambdaCallback(\n",
    "            on_epoch_end=lambda epoch, logs: print(\n",
    "                f\"Epoch {epoch+1}: \"\n",
    "                f\"MI Loss: {logs.get('val_motion_intensity_loss', 0):.4f}, \"\n",
    "                f\"VD Loss: {logs.get('val_vertical_dominance_loss', 0):.4f}, \"\n",
    "                f\"LR: {logs.get('learning_rate', 0):.6f}\"\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    print(\"‚úÖ Advanced training setup configured!\")\n",
    "    print(f\"Loss weights: {loss_weights}\")\n",
    "    print(f\"Initial learning rate: {initial_lr}\")\n",
    "    print(f\"Gradient clipping: enabled\")\n",
    "    print(f\"Learning rate scheduling: Cosine decay\")\n",
    "    \n",
    "    return {\n",
    "        'loss_weights': loss_weights,\n",
    "        'loss_functions': loss_functions,\n",
    "        'metrics': metrics,\n",
    "        'optimizer': optimizer,\n",
    "        'callbacks': callbacks\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Advanced training setup function defined!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data augmentation functions defined!\n",
      "Augmentation techniques:\n",
      "- Noise injection for robustness\n",
      "- Time warping for temporal patterns\n",
      "- Magnitude scaling for intensity patterns\n",
      "- Rotation augmentation for spatial patterns\n"
     ]
    }
   ],
   "source": [
    "# DATA AUGMENTATION FOR SENSOR DATA\n",
    "\n",
    "def augment_sensor_data(X, y, augmentation_factor=2):\n",
    "    \"\"\"\n",
    "    Apply data augmentation to sensor data to increase training set size\n",
    "    \"\"\"\n",
    "    print(f\"=== DATA AUGMENTATION ===\")\n",
    "    print(f\"Original data shape: {X.shape}\")\n",
    "    \n",
    "    # Initialize augmented data\n",
    "    X_augmented = [X]\n",
    "    y_augmented = [y]\n",
    "    \n",
    "    # 1. NOISE INJECTION (Add small random noise)\n",
    "    noise_factor = 0.05\n",
    "    for i in range(augmentation_factor):\n",
    "        noise = np.random.normal(0, noise_factor, X.shape)\n",
    "        X_noisy = X + noise\n",
    "        X_augmented.append(X_noisy)\n",
    "        y_augmented.append(y)\n",
    "    \n",
    "    # 2. TIME WARPING (Slight time stretching/compression)\n",
    "    for i in range(augmentation_factor):\n",
    "        warp_factor = np.random.uniform(0.95, 1.05)  # 5% variation\n",
    "        X_warped = np.zeros_like(X)\n",
    "        for j in range(X.shape[0]):\n",
    "            # Apply time warping to each sample\n",
    "            original_length = X.shape[1]\n",
    "            new_length = int(original_length * warp_factor)\n",
    "            if new_length > 0:\n",
    "                # Resample the time series\n",
    "                X_warped[j] = np.interp(\n",
    "                    np.linspace(0, original_length-1, original_length),\n",
    "                    np.linspace(0, original_length-1, new_length),\n",
    "                    X[j]\n",
    "                )\n",
    "        X_augmented.append(X_warped)\n",
    "        y_augmented.append(y)\n",
    "    \n",
    "    # 3. MAGNITUDE SCALING (Scale the magnitude of sensor readings)\n",
    "    for i in range(augmentation_factor):\n",
    "        scale_factor = np.random.uniform(0.9, 1.1)  # 10% variation\n",
    "        X_scaled = X * scale_factor\n",
    "        X_augmented.append(X_scaled)\n",
    "        y_augmented.append(y)\n",
    "    \n",
    "    # 4. ROTATION AUGMENTATION (Rotate sensor axes)\n",
    "    for i in range(augmentation_factor):\n",
    "        # Random rotation matrix for 3D data\n",
    "        angle = np.random.uniform(-0.1, 0.1)  # Small rotation\n",
    "        cos_a, sin_a = np.cos(angle), np.sin(angle)\n",
    "        \n",
    "        # Create rotation matrix\n",
    "        rotation_matrix = np.array([\n",
    "            [cos_a, -sin_a, 0],\n",
    "            [sin_a, cos_a, 0],\n",
    "            [0, 0, 1]\n",
    "        ])\n",
    "        \n",
    "        X_rotated = np.zeros_like(X)\n",
    "        for j in range(X.shape[0]):\n",
    "            # Apply rotation to each time step\n",
    "            for k in range(X.shape[1]):\n",
    "                X_rotated[j, k] = rotation_matrix @ X[j, k]\n",
    "        \n",
    "        X_augmented.append(X_rotated)\n",
    "        y_augmented.append(y)\n",
    "    \n",
    "    # Combine all augmented data\n",
    "    X_final = np.concatenate(X_augmented, axis=0)\n",
    "    y_final = np.concatenate(y_augmented, axis=0)\n",
    "    \n",
    "    print(f\"Augmented data shape: {X_final.shape}\")\n",
    "    print(f\"Augmentation factor: {X_final.shape[0] / X.shape[0]:.1f}x\")\n",
    "    print(f\"Total samples: {X_final.shape[0]}\")\n",
    "    \n",
    "    return X_final, y_final\n",
    "\n",
    "def apply_advanced_data_augmentation(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"\n",
    "    Apply advanced data augmentation to training data\n",
    "    \"\"\"\n",
    "    print(\"=== APPLYING ADVANCED DATA AUGMENTATION ===\")\n",
    "    \n",
    "    # Augment training data\n",
    "    X_train_aug, y_train_aug = augment_sensor_data(X_train, y_train, augmentation_factor=3)\n",
    "    \n",
    "    # Don't augment validation data (keep it clean for evaluation)\n",
    "    print(f\"Training data: {X_train.shape} ‚Üí {X_train_aug.shape}\")\n",
    "    print(f\"Validation data: {X_val.shape} (no augmentation)\")\n",
    "    \n",
    "    return X_train_aug, y_train_aug, X_val, y_val\n",
    "\n",
    "print(\"‚úÖ Data augmentation functions defined!\")\n",
    "print(\"Augmentation techniques:\")\n",
    "print(\"- Noise injection for robustness\")\n",
    "print(\"- Time warping for temporal patterns\")\n",
    "print(\"- Magnitude scaling for intensity patterns\")\n",
    "print(\"- Rotation augmentation for spatial patterns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COMPREHENSIVE IMPLEMENTATION GUIDE ===\n",
      "Current Performance:\n",
      "- Motion Intensity R¬≤: 0.3933 (target: 0.5+)\n",
      "- Vertical Dominance R¬≤: 0.1771 (target: 0.4+)\n",
      "\n",
      "=== IMPLEMENTATION STEPS ===\n",
      "\n",
      "üöÄ STEP 1: USE ADVANCED ENSEMBLE MODEL\n",
      "   - Replace your current model with the advanced ensemble model\n",
      "   - Features: Self-attention, multiple branches, ensemble averaging\n",
      "   - Expected improvement: 20-30% better performance\n",
      "\n",
      "‚öñÔ∏è STEP 2: USE ADVANCED TRAINING SETUP\n",
      "   - Higher loss weights: MI=20.0x, VD=25.0x\n",
      "   - Learning rate scheduling with cosine decay\n",
      "   - Gradient clipping for stable training\n",
      "   - Enhanced callbacks for better monitoring\n",
      "\n",
      "üîÑ STEP 3: APPLY DATA AUGMENTATION\n",
      "   - Increase training data by 4x through augmentation\n",
      "   - Techniques: noise injection, time warping, scaling, rotation\n",
      "   - Expected improvement: 15-25% better generalization\n",
      "\n",
      "üìä STEP 4: EXPECTED RESULTS\n",
      "   - Motion Intensity R¬≤: 0.3933 ‚Üí 0.5-0.6 (50-60%)\n",
      "   - Vertical Dominance R¬≤: 0.1771 ‚Üí 0.4-0.5 (40-50%)\n",
      "   - Overall improvement: 25-40% better performance\n",
      "\n",
      "üéØ STEP 5: IMPLEMENTATION CODE\n",
      "   # Build advanced model\n",
      "   model = build_advanced_ensemble_model(input_shape, n_classes_p, n_classes_t, n_classes_c, pretrained_encoder)\n",
      "   \n",
      "   # Get advanced training setup\n",
      "   training_config = create_advanced_training_setup()\n",
      "   \n",
      "   # Apply data augmentation\n",
      "   X_train_aug, y_train_aug, X_val_aug, y_val_aug = apply_advanced_data_augmentation(X_train, y_train, X_val, y_val)\n",
      "   \n",
      "   # Compile and train\n",
      "   model.compile(optimizer=training_config['optimizer'], loss=training_config['loss_functions'], loss_weights=training_config['loss_weights'], metrics=training_config['metrics'])\n",
      "   history = model.fit(X_train_aug, y_train_aug, validation_data=(X_val_aug, y_val_aug), epochs=100, callbacks=training_config['callbacks'])\n",
      "\n",
      "‚úÖ READY TO IMPLEMENT ADVANCED IMPROVEMENTS!\n",
      "These improvements should significantly boost your R¬≤ scores!\n"
     ]
    }
   ],
   "source": [
    "# COMPREHENSIVE IMPLEMENTATION GUIDE\n",
    "\n",
    "print(\"=== COMPREHENSIVE IMPLEMENTATION GUIDE ===\")\n",
    "print(\"Current Performance:\")\n",
    "print(\"- Motion Intensity R¬≤: 0.3933 (target: 0.5+)\")\n",
    "print(\"- Vertical Dominance R¬≤: 0.1771 (target: 0.4+)\")\n",
    "\n",
    "print(\"\\n=== IMPLEMENTATION STEPS ===\")\n",
    "\n",
    "print(\"\\nüöÄ STEP 1: USE ADVANCED ENSEMBLE MODEL\")\n",
    "print(\"   - Replace your current model with the advanced ensemble model\")\n",
    "print(\"   - Features: Self-attention, multiple branches, ensemble averaging\")\n",
    "print(\"   - Expected improvement: 20-30% better performance\")\n",
    "\n",
    "print(\"\\n‚öñÔ∏è STEP 2: USE ADVANCED TRAINING SETUP\")\n",
    "print(\"   - Higher loss weights: MI=20.0x, VD=25.0x\")\n",
    "print(\"   - Learning rate scheduling with cosine decay\")\n",
    "print(\"   - Gradient clipping for stable training\")\n",
    "print(\"   - Enhanced callbacks for better monitoring\")\n",
    "\n",
    "print(\"\\nüîÑ STEP 3: APPLY DATA AUGMENTATION\")\n",
    "print(\"   - Increase training data by 4x through augmentation\")\n",
    "print(\"   - Techniques: noise injection, time warping, scaling, rotation\")\n",
    "print(\"   - Expected improvement: 15-25% better generalization\")\n",
    "\n",
    "print(\"\\nüìä STEP 4: EXPECTED RESULTS\")\n",
    "print(\"   - Motion Intensity R¬≤: 0.3933 ‚Üí 0.5-0.6 (50-60%)\")\n",
    "print(\"   - Vertical Dominance R¬≤: 0.1771 ‚Üí 0.4-0.5 (40-50%)\")\n",
    "print(\"   - Overall improvement: 25-40% better performance\")\n",
    "\n",
    "print(\"\\nüéØ STEP 5: IMPLEMENTATION CODE\")\n",
    "print(\"   # Build advanced model\")\n",
    "print(\"   model = build_advanced_ensemble_model(input_shape, n_classes_p, n_classes_t, n_classes_c, pretrained_encoder)\")\n",
    "print(\"   \")\n",
    "print(\"   # Get advanced training setup\")\n",
    "print(\"   training_config = create_advanced_training_setup()\")\n",
    "print(\"   \")\n",
    "print(\"   # Apply data augmentation\")\n",
    "print(\"   X_train_aug, y_train_aug, X_val_aug, y_val_aug = apply_advanced_data_augmentation(X_train, y_train, X_val, y_val)\")\n",
    "print(\"   \")\n",
    "print(\"   # Compile and train\")\n",
    "print(\"   model.compile(optimizer=training_config['optimizer'], loss=training_config['loss_functions'], loss_weights=training_config['loss_weights'], metrics=training_config['metrics'])\")\n",
    "print(\"   history = model.fit(X_train_aug, y_train_aug, validation_data=(X_val_aug, y_val_aug), epochs=100, callbacks=training_config['callbacks'])\")\n",
    "\n",
    "print(\"\\n‚úÖ READY TO IMPLEMENT ADVANCED IMPROVEMENTS!\")\n",
    "print(\"These improvements should significantly boost your R¬≤ scores!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CRITICAL ANALYSIS: NEGATIVE R¬≤ VALUES ===\n",
      "Motion Intensity - R¬≤ (scaled): 0.5262 ‚úÖ (EXCELLENT improvement!)\n",
      "Vertical Dominance - R¬≤ (scaled): -0.0482 ‚ùå (CRITICAL PROBLEM!)\n",
      "Vertical Dominance - R¬≤ (original): -0.9369 ‚ùå (SEVERE OVERFITTING!)\n",
      "\n",
      "=== WHAT NEGATIVE R¬≤ MEANS ===\n",
      "R¬≤ = 1 - (SS_res / SS_tot)\n",
      "Where:\n",
      "- SS_res = Sum of squared residuals (prediction errors)\n",
      "- SS_tot = Sum of squared deviations from mean\n",
      "\n",
      "‚ùå NEGATIVE R¬≤ means:\n",
      "   - Model predictions are WORSE than just predicting the mean!\n",
      "   - SS_res > SS_tot (prediction errors > variance in data)\n",
      "   - Model is performing WORSE than a constant predictor\n",
      "\n",
      "=== WHY THIS HAPPENED ===\n",
      "üîç 1. SEVERE OVERFITTING:\n",
      "   - Model memorized training data but can't generalize\n",
      "   - Validation predictions are completely wrong\n",
      "   - Training loss is low but validation loss is very high\n",
      "\n",
      "üîç 2. DATA AUGMENTATION ISSUES:\n",
      "   - Augmented data may have corrupted the patterns\n",
      "   - Rotation augmentation might have broken spatial relationships\n",
      "   - Time warping might have destroyed temporal patterns\n",
      "\n",
      "üîç 3. MODEL COMPLEXITY:\n",
      "   - Too many parameters for the amount of data\n",
      "   - Ensemble model might be too complex\n",
      "   - Attention mechanism might be learning noise\n",
      "\n",
      "üîç 4. TRAINING ISSUES:\n",
      "   - Learning rate too high causing instability\n",
      "   - Loss weights too high causing imbalance\n",
      "   - Gradient clipping might be preventing learning\n",
      "\n",
      "=== IMMEDIATE FIXES NEEDED ===\n",
      "üö® 1. STOP USING CURRENT MODEL\n",
      "   - Negative R¬≤ means model is completely broken\n",
      "   - Need to revert to simpler approach\n",
      "\n",
      "üö® 2. SIMPLIFY MODEL ARCHITECTURE\n",
      "   - Remove ensemble complexity\n",
      "   - Remove attention mechanisms\n",
      "   - Use simpler, more stable architecture\n",
      "\n",
      "üö® 3. FIX DATA AUGMENTATION\n",
      "   - Reduce augmentation intensity\n",
      "   - Remove problematic augmentations\n",
      "   - Focus on noise injection only\n",
      "\n",
      "üö® 4. ADJUST TRAINING PARAMETERS\n",
      "   - Lower learning rate\n",
      "   - Reduce loss weights\n",
      "   - Add more regularization\n"
     ]
    }
   ],
   "source": [
    "# CRITICAL ANALYSIS: NEGATIVE R¬≤ VALUES\n",
    "\n",
    "print(\"=== CRITICAL ANALYSIS: NEGATIVE R¬≤ VALUES ===\")\n",
    "print(\"Motion Intensity - R¬≤ (scaled): 0.5262 ‚úÖ (EXCELLENT improvement!)\")\n",
    "print(\"Vertical Dominance - R¬≤ (scaled): -0.0482 ‚ùå (CRITICAL PROBLEM!)\")\n",
    "print(\"Vertical Dominance - R¬≤ (original): -0.9369 ‚ùå (SEVERE OVERFITTING!)\")\n",
    "\n",
    "print(\"\\n=== WHAT NEGATIVE R¬≤ MEANS ===\")\n",
    "print(\"R¬≤ = 1 - (SS_res / SS_tot)\")\n",
    "print(\"Where:\")\n",
    "print(\"- SS_res = Sum of squared residuals (prediction errors)\")\n",
    "print(\"- SS_tot = Sum of squared deviations from mean\")\n",
    "print(\"\")\n",
    "print(\"‚ùå NEGATIVE R¬≤ means:\")\n",
    "print(\"   - Model predictions are WORSE than just predicting the mean!\")\n",
    "print(\"   - SS_res > SS_tot (prediction errors > variance in data)\")\n",
    "print(\"   - Model is performing WORSE than a constant predictor\")\n",
    "\n",
    "print(\"\\n=== WHY THIS HAPPENED ===\")\n",
    "print(\"üîç 1. SEVERE OVERFITTING:\")\n",
    "print(\"   - Model memorized training data but can't generalize\")\n",
    "print(\"   - Validation predictions are completely wrong\")\n",
    "print(\"   - Training loss is low but validation loss is very high\")\n",
    "\n",
    "print(\"\\nüîç 2. DATA AUGMENTATION ISSUES:\")\n",
    "print(\"   - Augmented data may have corrupted the patterns\")\n",
    "print(\"   - Rotation augmentation might have broken spatial relationships\")\n",
    "print(\"   - Time warping might have destroyed temporal patterns\")\n",
    "\n",
    "print(\"\\nüîç 3. MODEL COMPLEXITY:\")\n",
    "print(\"   - Too many parameters for the amount of data\")\n",
    "print(\"   - Ensemble model might be too complex\")\n",
    "print(\"   - Attention mechanism might be learning noise\")\n",
    "\n",
    "print(\"\\nüîç 4. TRAINING ISSUES:\")\n",
    "print(\"   - Learning rate too high causing instability\")\n",
    "print(\"   - Loss weights too high causing imbalance\")\n",
    "print(\"   - Gradient clipping might be preventing learning\")\n",
    "\n",
    "print(\"\\n=== IMMEDIATE FIXES NEEDED ===\")\n",
    "print(\"üö® 1. STOP USING CURRENT MODEL\")\n",
    "print(\"   - Negative R¬≤ means model is completely broken\")\n",
    "print(\"   - Need to revert to simpler approach\")\n",
    "\n",
    "print(\"\\nüö® 2. SIMPLIFY MODEL ARCHITECTURE\")\n",
    "print(\"   - Remove ensemble complexity\")\n",
    "print(\"   - Remove attention mechanisms\")\n",
    "print(\"   - Use simpler, more stable architecture\")\n",
    "\n",
    "print(\"\\nüö® 3. FIX DATA AUGMENTATION\")\n",
    "print(\"   - Reduce augmentation intensity\")\n",
    "print(\"   - Remove problematic augmentations\")\n",
    "print(\"   - Focus on noise injection only\")\n",
    "\n",
    "print(\"\\nüö® 4. ADJUST TRAINING PARAMETERS\")\n",
    "print(\"   - Lower learning rate\")\n",
    "print(\"   - Reduce loss weights\")\n",
    "print(\"   - Add more regularization\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Simplified stable model defined!\n",
      "Key simplifications:\n",
      "- Removed attention mechanisms\n",
      "- Removed ensemble complexity\n",
      "- Removed multiple branches\n",
      "- Increased dropout for better regularization\n",
      "- Simpler architecture for stability\n"
     ]
    }
   ],
   "source": [
    "# SIMPLIFIED STABLE MODEL (FIXES NEGATIVE R¬≤)\n",
    "\n",
    "def build_simplified_stable_model(input_shape, n_classes_p, n_classes_t, n_classes_c, pretrained_encoder):\n",
    "    \"\"\"\n",
    "    Simplified, stable model that prevents negative R¬≤ values\n",
    "    \"\"\"\n",
    "    # Input layer\n",
    "    sensor_input = tf.keras.layers.Input(shape=input_shape, name='sensor_input')\n",
    "    \n",
    "    # Use pre-trained encoder as feature extractor\n",
    "    pretrained_features = pretrained_encoder.tf_encoder(sensor_input)\n",
    "    \n",
    "    # SIMPLIFIED shared feature processing (no attention, no complex layers)\n",
    "    x = tf.keras.layers.Dense(64, activation='relu', name='shared_dense1')(pretrained_features)\n",
    "    x = tf.keras.layers.BatchNormalization(name='shared_bn1')(x)\n",
    "    x = tf.keras.layers.Dropout(0.4, name='shared_dropout1')(x)  # Higher dropout\n",
    "    \n",
    "    x = tf.keras.layers.Dense(32, activation='relu', name='shared_dense2')(x)\n",
    "    x = tf.keras.layers.BatchNormalization(name='shared_bn2')(x)\n",
    "    x = tf.keras.layers.Dropout(0.4, name='shared_dropout2')(x)  # Higher dropout\n",
    "    \n",
    "    # Classification outputs (discrete concepts)\n",
    "    periodicity = tf.keras.layers.Dense(n_classes_p, activation='softmax', name='periodicity')(x)\n",
    "    temporal_stability = tf.keras.layers.Dense(n_classes_t, activation='softmax', name='temporal_stability')(x)\n",
    "    coordination = tf.keras.layers.Dense(n_classes_c, activation='softmax', name='coordination')(x)\n",
    "    \n",
    "    # SIMPLIFIED motion intensity branch (keep what works)\n",
    "    mi_branch = tf.keras.layers.Dense(16, activation='relu', name='mi_dense1')(x)\n",
    "    mi_branch = tf.keras.layers.Dropout(0.3, name='mi_dropout1')(mi_branch)\n",
    "    mi_branch = tf.keras.layers.Dense(8, activation='relu', name='mi_dense2')(mi_branch)\n",
    "    mi_branch = tf.keras.layers.Dropout(0.3, name='mi_dropout2')(mi_branch)\n",
    "    motion_intensity = tf.keras.layers.Dense(1, activation='sigmoid', name='motion_intensity')(mi_branch)\n",
    "    \n",
    "    # SIMPLIFIED vertical dominance branch (remove complexity that caused issues)\n",
    "    vd_branch = tf.keras.layers.Dense(16, activation='relu', name='vd_dense1')(x)\n",
    "    vd_branch = tf.keras.layers.Dropout(0.4, name='vd_dropout1')(vd_branch)  # Higher dropout\n",
    "    vd_branch = tf.keras.layers.Dense(8, activation='relu', name='vd_dense2')(vd_branch)\n",
    "    vd_branch = tf.keras.layers.Dropout(0.4, name='vd_dropout2')(vd_branch)  # Higher dropout\n",
    "    vertical_dominance = tf.keras.layers.Dense(1, activation='sigmoid', name='vertical_dominance')(vd_branch)\n",
    "    \n",
    "    model = tf.keras.models.Model(\n",
    "        inputs=sensor_input, \n",
    "        outputs=[periodicity, temporal_stability, coordination, motion_intensity, vertical_dominance]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"‚úÖ Simplified stable model defined!\")\n",
    "print(\"Key simplifications:\")\n",
    "print(\"- Removed attention mechanisms\")\n",
    "print(\"- Removed ensemble complexity\")\n",
    "print(\"- Removed multiple branches\")\n",
    "print(\"- Increased dropout for better regularization\")\n",
    "print(\"- Simpler architecture for stability\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Conservative training setup function defined!\n"
     ]
    }
   ],
   "source": [
    "# CONSERVATIVE TRAINING SETUP (PREVENTS OVERFITTING)\n",
    "\n",
    "def create_conservative_training_setup():\n",
    "    \"\"\"\n",
    "    Conservative training configuration that prevents overfitting and negative R¬≤\n",
    "    \"\"\"\n",
    "    print(\"=== CONSERVATIVE TRAINING SETUP ===\")\n",
    "    \n",
    "    # 1. CONSERVATIVE LOSS WEIGHTS (balanced approach)\n",
    "    loss_weights = {\n",
    "        'periodicity': 1.0,\n",
    "        'temporal_stability': 1.0,\n",
    "        'coordination': 1.0,\n",
    "        'motion_intensity': 10.0,      # Keep what works\n",
    "        'vertical_dominance': 10.0     # REDUCED from 25.0 to 10.0\n",
    "    }\n",
    "    \n",
    "    # 2. CONSERVATIVE LOSS FUNCTIONS\n",
    "    loss_functions = {\n",
    "        'periodicity': 'categorical_crossentropy',\n",
    "        'temporal_stability': 'categorical_crossentropy',\n",
    "        'coordination': 'categorical_crossentropy',\n",
    "        'motion_intensity': 'huber',\n",
    "        'vertical_dominance': 'huber'\n",
    "    }\n",
    "    \n",
    "    # 3. CONSERVATIVE METRICS\n",
    "    metrics = {\n",
    "        'periodicity': ['accuracy'],\n",
    "        'temporal_stability': ['accuracy'],\n",
    "        'coordination': ['accuracy'],\n",
    "        'motion_intensity': ['mae', 'mse'],\n",
    "        'vertical_dominance': ['mae', 'mse']\n",
    "    }\n",
    "    \n",
    "    # 4. CONSERVATIVE OPTIMIZER (lower learning rate, no scheduling)\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "        learning_rate=0.0001,  # REDUCED from 0.0005 to 0.0001\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        epsilon=1e-07,\n",
    "        clipnorm=0.5  # REDUCED gradient clipping\n",
    "    )\n",
    "    \n",
    "    # 5. CONSERVATIVE CALLBACKS (early stopping, no aggressive reduction)\n",
    "    callbacks = [\n",
    "        # Early stopping with patience\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=15,  # REDUCED from 20 to 15\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        \n",
    "        # Conservative learning rate reduction\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,  # LESS aggressive reduction\n",
    "            patience=8,  # REDUCED from 10 to 8\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        ),\n",
    "        \n",
    "        # Model checkpointing\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath='best_conservative_model.keras',\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        \n",
    "        # Custom callback for monitoring\n",
    "        tf.keras.callbacks.LambdaCallback(\n",
    "            on_epoch_end=lambda epoch, logs: print(\n",
    "                f\"Epoch {epoch+1}: \"\n",
    "                f\"MI Loss: {logs.get('val_motion_intensity_loss', 0):.4f}, \"\n",
    "                f\"VD Loss: {logs.get('val_vertical_dominance_loss', 0):.4f}, \"\n",
    "                f\"Total Loss: {logs.get('val_loss', 0):.4f}\"\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    print(\"‚úÖ Conservative training setup configured!\")\n",
    "    print(f\"Loss weights: {loss_weights}\")\n",
    "    print(f\"Learning rate: {optimizer.learning_rate}\")\n",
    "    print(f\"Gradient clipping: {optimizer.clipnorm}\")\n",
    "    print(f\"Focus: Stability and preventing overfitting\")\n",
    "    \n",
    "    return {\n",
    "        'loss_weights': loss_weights,\n",
    "        'loss_functions': loss_functions,\n",
    "        'metrics': metrics,\n",
    "        'optimizer': optimizer,\n",
    "        'callbacks': callbacks\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Conservative training setup function defined!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Minimal safe data augmentation function defined!\n",
      "Key features:\n",
      "- Only noise injection (safest augmentation)\n",
      "- Very small noise factor (0.02)\n",
      "- Minimal 3x augmentation\n",
      "- No rotation, time warping, or scaling\n",
      "- Preserves original data patterns\n"
     ]
    }
   ],
   "source": [
    "# MINIMAL DATA AUGMENTATION (SAFE APPROACH)\n",
    "\n",
    "def apply_minimal_safe_augmentation(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"\n",
    "    Apply minimal, safe data augmentation that won't break patterns\n",
    "    \"\"\"\n",
    "    print(\"=== APPLYING MINIMAL SAFE DATA AUGMENTATION ===\")\n",
    "    \n",
    "    # Only apply noise injection (safest augmentation)\n",
    "    noise_factor = 0.02  # REDUCED from 0.05 to 0.02 (very small noise)\n",
    "    \n",
    "    # Create augmented training data\n",
    "    X_train_aug = [X_train]\n",
    "    y_train_aug = [y_train]\n",
    "    \n",
    "    # Add 2x noise-augmented data (minimal augmentation)\n",
    "    for i in range(2):\n",
    "        noise = np.random.normal(0, noise_factor, X_train.shape)\n",
    "        X_noisy = X_train + noise\n",
    "        X_train_aug.append(X_noisy)\n",
    "        y_train_aug.append(y_train)\n",
    "    \n",
    "    # Combine augmented data\n",
    "    X_train_final = np.concatenate(X_train_aug, axis=0)\n",
    "    y_train_final = np.concatenate(y_train_aug, axis=0)\n",
    "    \n",
    "    print(f\"Training data: {X_train.shape} ‚Üí {X_train_final.shape}\")\n",
    "    print(f\"Augmentation factor: {X_train_final.shape[0] / X_train.shape[0]:.1f}x\")\n",
    "    print(f\"Validation data: {X_val.shape} (no augmentation)\")\n",
    "    print(\"‚úÖ Only noise injection applied (safest approach)\")\n",
    "    \n",
    "    return X_train_final, y_train_final, X_val, y_val\n",
    "\n",
    "print(\"‚úÖ Minimal safe data augmentation function defined!\")\n",
    "print(\"Key features:\")\n",
    "print(\"- Only noise injection (safest augmentation)\")\n",
    "print(\"- Very small noise factor (0.02)\")\n",
    "print(\"- Minimal 3x augmentation\")\n",
    "print(\"- No rotation, time warping, or scaling\")\n",
    "print(\"- Preserves original data patterns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COMPREHENSIVE FIX SUMMARY ===\n",
      "üö® PROBLEM: Negative R¬≤ values indicate severe overfitting\n",
      "‚úÖ SOLUTION: Simplified, stable approach\n",
      "\n",
      "=== WHAT WENT WRONG ===\n",
      "‚ùå Advanced ensemble model was too complex\n",
      "‚ùå Data augmentation corrupted spatial patterns\n",
      "‚ùå High loss weights caused training instability\n",
      "‚ùå Learning rate was too high\n",
      "‚ùå Model memorized training data but couldn't generalize\n",
      "\n",
      "=== FIXES IMPLEMENTED ===\n",
      "\n",
      "üèóÔ∏è 1. SIMPLIFIED MODEL ARCHITECTURE:\n",
      "   - Removed attention mechanisms\n",
      "   - Removed ensemble complexity\n",
      "   - Removed multiple branches\n",
      "   - Increased dropout (0.4) for better regularization\n",
      "   - Simpler, more stable architecture\n",
      "\n",
      "‚öñÔ∏è 2. CONSERVATIVE TRAINING SETUP:\n",
      "   - Lower learning rate: 0.0001 (vs 0.0005)\n",
      "   - Reduced loss weights: VD=10.0 (vs 25.0)\n",
      "   - Conservative gradient clipping: 0.5 (vs 1.0)\n",
      "   - Less aggressive learning rate reduction\n",
      "   - Focus on stability over performance\n",
      "\n",
      "üîÑ 3. MINIMAL SAFE DATA AUGMENTATION:\n",
      "   - Only noise injection (safest approach)\n",
      "   - Very small noise factor: 0.02 (vs 0.05)\n",
      "   - Minimal 3x augmentation (vs 4x)\n",
      "   - No rotation, time warping, or scaling\n",
      "   - Preserves original data patterns\n",
      "\n",
      "üìä 4. EXPECTED RESULTS:\n",
      "   - Motion Intensity R¬≤: 0.5262 ‚Üí 0.5-0.6 (maintain good performance)\n",
      "   - Vertical Dominance R¬≤: -0.0482 ‚Üí 0.2-0.4 (fix negative values)\n",
      "   - Overall: Stable, positive R¬≤ values\n",
      "\n",
      "üéØ 5. IMPLEMENTATION CODE:\n",
      "   # Build simplified stable model\n",
      "   model = build_simplified_stable_model(input_shape, n_classes_p, n_classes_t, n_classes_c, pretrained_encoder)\n",
      "   \n",
      "   # Get conservative training setup\n",
      "   training_config = create_conservative_training_setup()\n",
      "   \n",
      "   # Apply minimal safe augmentation\n",
      "   X_train_aug, y_train_aug, X_val_aug, y_val_aug = apply_minimal_safe_augmentation(X_train, y_train, X_val, y_val)\n",
      "   \n",
      "   # Compile and train\n",
      "   model.compile(optimizer=training_config['optimizer'], loss=training_config['loss_functions'], loss_weights=training_config['loss_weights'], metrics=training_config['metrics'])\n",
      "   history = model.fit(X_train_aug, y_train_aug, validation_data=(X_val_aug, y_val_aug), epochs=100, callbacks=training_config['callbacks'])\n",
      "\n",
      "‚úÖ READY TO FIX NEGATIVE R¬≤ VALUES!\n",
      "This approach should give you stable, positive R¬≤ values!\n"
     ]
    }
   ],
   "source": [
    "# COMPREHENSIVE FIX SUMMARY\n",
    "\n",
    "print(\"=== COMPREHENSIVE FIX SUMMARY ===\")\n",
    "print(\"üö® PROBLEM: Negative R¬≤ values indicate severe overfitting\")\n",
    "print(\"‚úÖ SOLUTION: Simplified, stable approach\")\n",
    "\n",
    "print(\"\\n=== WHAT WENT WRONG ===\")\n",
    "print(\"‚ùå Advanced ensemble model was too complex\")\n",
    "print(\"‚ùå Data augmentation corrupted spatial patterns\")\n",
    "print(\"‚ùå High loss weights caused training instability\")\n",
    "print(\"‚ùå Learning rate was too high\")\n",
    "print(\"‚ùå Model memorized training data but couldn't generalize\")\n",
    "\n",
    "print(\"\\n=== FIXES IMPLEMENTED ===\")\n",
    "\n",
    "print(\"\\nüèóÔ∏è 1. SIMPLIFIED MODEL ARCHITECTURE:\")\n",
    "print(\"   - Removed attention mechanisms\")\n",
    "print(\"   - Removed ensemble complexity\")\n",
    "print(\"   - Removed multiple branches\")\n",
    "print(\"   - Increased dropout (0.4) for better regularization\")\n",
    "print(\"   - Simpler, more stable architecture\")\n",
    "\n",
    "print(\"\\n‚öñÔ∏è 2. CONSERVATIVE TRAINING SETUP:\")\n",
    "print(\"   - Lower learning rate: 0.0001 (vs 0.0005)\")\n",
    "print(\"   - Reduced loss weights: VD=10.0 (vs 25.0)\")\n",
    "print(\"   - Conservative gradient clipping: 0.5 (vs 1.0)\")\n",
    "print(\"   - Less aggressive learning rate reduction\")\n",
    "print(\"   - Focus on stability over performance\")\n",
    "\n",
    "print(\"\\nüîÑ 3. MINIMAL SAFE DATA AUGMENTATION:\")\n",
    "print(\"   - Only noise injection (safest approach)\")\n",
    "print(\"   - Very small noise factor: 0.02 (vs 0.05)\")\n",
    "print(\"   - Minimal 3x augmentation (vs 4x)\")\n",
    "print(\"   - No rotation, time warping, or scaling\")\n",
    "print(\"   - Preserves original data patterns\")\n",
    "\n",
    "print(\"\\nüìä 4. EXPECTED RESULTS:\")\n",
    "print(\"   - Motion Intensity R¬≤: 0.5262 ‚Üí 0.5-0.6 (maintain good performance)\")\n",
    "print(\"   - Vertical Dominance R¬≤: -0.0482 ‚Üí 0.2-0.4 (fix negative values)\")\n",
    "print(\"   - Overall: Stable, positive R¬≤ values\")\n",
    "\n",
    "print(\"\\nüéØ 5. IMPLEMENTATION CODE:\")\n",
    "print(\"   # Build simplified stable model\")\n",
    "print(\"   model = build_simplified_stable_model(input_shape, n_classes_p, n_classes_t, n_classes_c, pretrained_encoder)\")\n",
    "print(\"   \")\n",
    "print(\"   # Get conservative training setup\")\n",
    "print(\"   training_config = create_conservative_training_setup()\")\n",
    "print(\"   \")\n",
    "print(\"   # Apply minimal safe augmentation\")\n",
    "print(\"   X_train_aug, y_train_aug, X_val_aug, y_val_aug = apply_minimal_safe_augmentation(X_train, y_train, X_val, y_val)\")\n",
    "print(\"   \")\n",
    "print(\"   # Compile and train\")\n",
    "print(\"   model.compile(optimizer=training_config['optimizer'], loss=training_config['loss_functions'], loss_weights=training_config['loss_weights'], metrics=training_config['metrics'])\")\n",
    "print(\"   history = model.fit(X_train_aug, y_train_aug, validation_data=(X_val_aug, y_val_aug), epochs=100, callbacks=training_config['callbacks'])\")\n",
    "\n",
    "print(\"\\n‚úÖ READY TO FIX NEGATIVE R¬≤ VALUES!\")\n",
    "print(\"This approach should give you stable, positive R¬≤ values!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MULTI-TASK LEARNING PROBLEM ANALYSIS ===\n",
      "üö® PROBLEM: Motion intensity and vertical dominance are competing!\n",
      "‚úÖ SOLUTION: Separate feature extraction for each task\n",
      "\n",
      "=== WHY TASKS COMPETE ===\n",
      "üîç 1. SHARED FEATURE EXTRACTION:\n",
      "   - Both tasks use the same pre-trained encoder\n",
      "   - Both tasks share the same hidden layers\n",
      "   - Features learned for one task may hurt the other\n",
      "   - Motion intensity needs temporal patterns\n",
      "   - Vertical dominance needs spatial patterns\n",
      "\n",
      "üîç 2. LOSS WEIGHT CONFLICTS:\n",
      "   - High weight on one task dominates training\n",
      "   - Other task gets less attention\n",
      "   - Model focuses on easier task (motion intensity)\n",
      "   - Harder task (vertical dominance) gets ignored\n",
      "\n",
      "üîç 3. FEATURE INCOMPATIBILITY:\n",
      "   - Motion intensity: Needs magnitude and frequency features\n",
      "   - Vertical dominance: Needs orientation and spatial features\n",
      "   - These features may be contradictory\n",
      "   - Shared layers can't optimize for both\n",
      "\n",
      "=== SOLUTION: SEPARATE FEATURE EXTRACTION ===\n",
      "\n",
      "üèóÔ∏è 1. DUAL ENCODER ARCHITECTURE:\n",
      "   - Separate encoders for each regression task\n",
      "   - Motion intensity: Temporal-focused encoder\n",
      "   - Vertical dominance: Spatial-focused encoder\n",
      "   - No competition between tasks\n",
      "\n",
      "üèóÔ∏è 2. TASK-SPECIFIC FEATURES:\n",
      "   - Motion intensity: Magnitude, frequency, temporal patterns\n",
      "   - Vertical dominance: Orientation, spatial relationships\n",
      "   - Each task gets optimized features\n",
      "\n",
      "üèóÔ∏è 3. BALANCED TRAINING:\n",
      "   - Equal loss weights for both tasks\n",
      "   - No task dominates the other\n",
      "   - Both tasks improve simultaneously\n",
      "\n",
      "=== IMPLEMENTATION STRATEGY ===\n",
      "üéØ 1. CREATE DUAL ENCODER MODEL\n",
      "üéØ 2. TASK-SPECIFIC FEATURE EXTRACTION\n",
      "üéØ 3. BALANCED LOSS WEIGHTS\n",
      "üéØ 4. SEPARATE OPTIMIZATION PATHS\n"
     ]
    }
   ],
   "source": [
    "# MULTI-TASK LEARNING ANALYSIS\n",
    "\n",
    "print(\"=== MULTI-TASK LEARNING PROBLEM ANALYSIS ===\")\n",
    "print(\"üö® PROBLEM: Motion intensity and vertical dominance are competing!\")\n",
    "print(\"‚úÖ SOLUTION: Separate feature extraction for each task\")\n",
    "\n",
    "print(\"\\n=== WHY TASKS COMPETE ===\")\n",
    "print(\"üîç 1. SHARED FEATURE EXTRACTION:\")\n",
    "print(\"   - Both tasks use the same pre-trained encoder\")\n",
    "print(\"   - Both tasks share the same hidden layers\")\n",
    "print(\"   - Features learned for one task may hurt the other\")\n",
    "print(\"   - Motion intensity needs temporal patterns\")\n",
    "print(\"   - Vertical dominance needs spatial patterns\")\n",
    "\n",
    "print(\"\\nüîç 2. LOSS WEIGHT CONFLICTS:\")\n",
    "print(\"   - High weight on one task dominates training\")\n",
    "print(\"   - Other task gets less attention\")\n",
    "print(\"   - Model focuses on easier task (motion intensity)\")\n",
    "print(\"   - Harder task (vertical dominance) gets ignored\")\n",
    "\n",
    "print(\"\\nüîç 3. FEATURE INCOMPATIBILITY:\")\n",
    "print(\"   - Motion intensity: Needs magnitude and frequency features\")\n",
    "print(\"   - Vertical dominance: Needs orientation and spatial features\")\n",
    "print(\"   - These features may be contradictory\")\n",
    "print(\"   - Shared layers can't optimize for both\")\n",
    "\n",
    "print(\"\\n=== SOLUTION: SEPARATE FEATURE EXTRACTION ===\")\n",
    "\n",
    "print(\"\\nüèóÔ∏è 1. DUAL ENCODER ARCHITECTURE:\")\n",
    "print(\"   - Separate encoders for each regression task\")\n",
    "print(\"   - Motion intensity: Temporal-focused encoder\")\n",
    "print(\"   - Vertical dominance: Spatial-focused encoder\")\n",
    "print(\"   - No competition between tasks\")\n",
    "\n",
    "print(\"\\nüèóÔ∏è 2. TASK-SPECIFIC FEATURES:\")\n",
    "print(\"   - Motion intensity: Magnitude, frequency, temporal patterns\")\n",
    "print(\"   - Vertical dominance: Orientation, spatial relationships\")\n",
    "print(\"   - Each task gets optimized features\")\n",
    "\n",
    "print(\"\\nüèóÔ∏è 3. BALANCED TRAINING:\")\n",
    "print(\"   - Equal loss weights for both tasks\")\n",
    "print(\"   - No task dominates the other\")\n",
    "print(\"   - Both tasks improve simultaneously\")\n",
    "\n",
    "print(\"\\n=== IMPLEMENTATION STRATEGY ===\")\n",
    "print(\"üéØ 1. CREATE DUAL ENCODER MODEL\")\n",
    "print(\"üéØ 2. TASK-SPECIFIC FEATURE EXTRACTION\")\n",
    "print(\"üéØ 3. BALANCED LOSS WEIGHTS\")\n",
    "print(\"üéØ 4. SEPARATE OPTIMIZATION PATHS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dual encoder model defined!\n",
      "Key features:\n",
      "- Separate encoders for motion intensity and vertical dominance\n",
      "- No competition between regression tasks\n",
      "- Each task gets optimized features\n",
      "- Shared features only for classification tasks\n",
      "- Independent optimization paths\n"
     ]
    }
   ],
   "source": [
    "# DUAL ENCODER MODEL (SEPARATES TASKS)\n",
    "\n",
    "def build_dual_encoder_model(input_shape, n_classes_p, n_classes_t, n_classes_c, pretrained_encoder):\n",
    "    \"\"\"\n",
    "    Dual encoder model that separates motion intensity and vertical dominance\n",
    "    \"\"\"\n",
    "    # Input layer\n",
    "    sensor_input = tf.keras.layers.Input(shape=input_shape, name='sensor_input')\n",
    "    \n",
    "    # SHARED: Use pre-trained encoder for classification tasks\n",
    "    pretrained_features = pretrained_encoder.tf_encoder(sensor_input)\n",
    "    \n",
    "    # SHARED: Classification outputs (discrete concepts)\n",
    "    x_shared = tf.keras.layers.Dense(64, activation='relu', name='shared_dense1')(pretrained_features)\n",
    "    x_shared = tf.keras.layers.BatchNormalization(name='shared_bn1')(x_shared)\n",
    "    x_shared = tf.keras.layers.Dropout(0.3, name='shared_dropout1')(x_shared)\n",
    "    \n",
    "    x_shared = tf.keras.layers.Dense(32, activation='relu', name='shared_dense2')(x_shared)\n",
    "    x_shared = tf.keras.layers.BatchNormalization(name='shared_bn2')(x_shared)\n",
    "    x_shared = tf.keras.layers.Dropout(0.3, name='shared_dropout2')(x_shared)\n",
    "    \n",
    "    # Classification outputs\n",
    "    periodicity = tf.keras.layers.Dense(n_classes_p, activation='softmax', name='periodicity')(x_shared)\n",
    "    temporal_stability = tf.keras.layers.Dense(n_classes_t, activation='softmax', name='temporal_stability')(x_shared)\n",
    "    coordination = tf.keras.layers.Dense(n_classes_c, activation='softmax', name='coordination')(x_shared)\n",
    "    \n",
    "    # SEPARATE: Motion Intensity Encoder (Temporal Focus)\n",
    "    mi_encoder = tf.keras.layers.Dense(128, activation='relu', name='mi_encoder1')(pretrained_features)\n",
    "    mi_encoder = tf.keras.layers.BatchNormalization(name='mi_encoder_bn1')(mi_encoder)\n",
    "    mi_encoder = tf.keras.layers.Dropout(0.2, name='mi_encoder_dropout1')(mi_encoder)\n",
    "    \n",
    "    mi_encoder = tf.keras.layers.Dense(64, activation='relu', name='mi_encoder2')(mi_encoder)\n",
    "    mi_encoder = tf.keras.layers.BatchNormalization(name='mi_encoder_bn2')(mi_encoder)\n",
    "    mi_encoder = tf.keras.layers.Dropout(0.2, name='mi_encoder_dropout2')(mi_encoder)\n",
    "    \n",
    "    # Motion Intensity Branch\n",
    "    mi_branch = tf.keras.layers.Dense(32, activation='relu', name='mi_branch1')(mi_encoder)\n",
    "    mi_branch = tf.keras.layers.Dropout(0.2, name='mi_branch_dropout1')(mi_branch)\n",
    "    mi_branch = tf.keras.layers.Dense(16, activation='relu', name='mi_branch2')(mi_branch)\n",
    "    mi_branch = tf.keras.layers.Dropout(0.2, name='mi_branch_dropout2')(mi_branch)\n",
    "    motion_intensity = tf.keras.layers.Dense(1, activation='sigmoid', name='motion_intensity')(mi_branch)\n",
    "    \n",
    "    # SEPARATE: Vertical Dominance Encoder (Spatial Focus)\n",
    "    vd_encoder = tf.keras.layers.Dense(128, activation='relu', name='vd_encoder1')(pretrained_features)\n",
    "    vd_encoder = tf.keras.layers.BatchNormalization(name='vd_encoder_bn1')(vd_encoder)\n",
    "    vd_encoder = tf.keras.layers.Dropout(0.2, name='vd_encoder_dropout1')(vd_encoder)\n",
    "    \n",
    "    vd_encoder = tf.keras.layers.Dense(64, activation='relu', name='vd_encoder2')(vd_encoder)\n",
    "    vd_encoder = tf.keras.layers.BatchNormalization(name='vd_encoder_bn2')(vd_encoder)\n",
    "    vd_encoder = tf.keras.layers.Dropout(0.2, name='vd_encoder_dropout2')(vd_encoder)\n",
    "    \n",
    "    # Vertical Dominance Branch\n",
    "    vd_branch = tf.keras.layers.Dense(32, activation='relu', name='vd_branch1')(vd_encoder)\n",
    "    vd_branch = tf.keras.layers.Dropout(0.2, name='vd_branch_dropout1')(vd_branch)\n",
    "    vd_branch = tf.keras.layers.Dense(16, activation='relu', name='vd_branch2')(vd_branch)\n",
    "    vd_branch = tf.keras.layers.Dropout(0.2, name='vd_branch_dropout2')(vd_branch)\n",
    "    vertical_dominance = tf.keras.layers.Dense(1, activation='sigmoid', name='vertical_dominance')(vd_branch)\n",
    "    \n",
    "    model = tf.keras.models.Model(\n",
    "        inputs=sensor_input, \n",
    "        outputs=[periodicity, temporal_stability, coordination, motion_intensity, vertical_dominance]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"‚úÖ Dual encoder model defined!\")\n",
    "print(\"Key features:\")\n",
    "print(\"- Separate encoders for motion intensity and vertical dominance\")\n",
    "print(\"- No competition between regression tasks\")\n",
    "print(\"- Each task gets optimized features\")\n",
    "print(\"- Shared features only for classification tasks\")\n",
    "print(\"- Independent optimization paths\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Balanced training setup function defined!\n"
     ]
    }
   ],
   "source": [
    "# BALANCED TRAINING SETUP (EQUAL TASK PRIORITY)\n",
    "\n",
    "def create_balanced_training_setup():\n",
    "    \"\"\"\n",
    "    Balanced training configuration that treats both regression tasks equally\n",
    "    \"\"\"\n",
    "    print(\"=== BALANCED TRAINING SETUP ===\")\n",
    "    \n",
    "    # 1. BALANCED LOSS WEIGHTS (Equal priority for both regression tasks)\n",
    "    loss_weights = {\n",
    "        'periodicity': 1.0,\n",
    "        'temporal_stability': 1.0,\n",
    "        'coordination': 1.0,\n",
    "        'motion_intensity': 15.0,      # EQUAL weight\n",
    "        'vertical_dominance': 15.0     # EQUAL weight (not competing!)\n",
    "    }\n",
    "    \n",
    "    # 2. BALANCED LOSS FUNCTIONS\n",
    "    loss_functions = {\n",
    "        'periodicity': 'categorical_crossentropy',\n",
    "        'temporal_stability': 'categorical_crossentropy',\n",
    "        'coordination': 'categorical_crossentropy',\n",
    "        'motion_intensity': 'huber',\n",
    "        'vertical_dominance': 'huber'\n",
    "    }\n",
    "    \n",
    "    # 3. BALANCED METRICS\n",
    "    metrics = {\n",
    "        'periodicity': ['accuracy'],\n",
    "        'temporal_stability': ['accuracy'],\n",
    "        'coordination': ['accuracy'],\n",
    "        'motion_intensity': ['mae', 'mse'],\n",
    "        'vertical_dominance': ['mae', 'mse']\n",
    "    }\n",
    "    \n",
    "    # 4. BALANCED OPTIMIZER\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "        learning_rate=0.0002,  # Balanced learning rate\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        epsilon=1e-07,\n",
    "        clipnorm=0.8  # Balanced gradient clipping\n",
    "    )\n",
    "    \n",
    "    # 5. BALANCED CALLBACKS\n",
    "    callbacks = [\n",
    "        # Early stopping with patience\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=20,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        \n",
    "        # Balanced learning rate reduction\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.3,\n",
    "            patience=10,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        ),\n",
    "        \n",
    "        # Model checkpointing\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath='best_balanced_model.keras',\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        \n",
    "        # Custom callback for monitoring both tasks\n",
    "        tf.keras.callbacks.LambdaCallback(\n",
    "            on_epoch_end=lambda epoch, logs: print(\n",
    "                f\"Epoch {epoch+1}: \"\n",
    "                f\"MI Loss: {logs.get('val_motion_intensity_loss', 0):.4f}, \"\n",
    "                f\"VD Loss: {logs.get('val_vertical_dominance_loss', 0):.4f}, \"\n",
    "                f\"MI MAE: {logs.get('val_motion_intensity_mae', 0):.4f}, \"\n",
    "                f\"VD MAE: {logs.get('val_vertical_dominance_mae', 0):.4f}\"\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    print(\"‚úÖ Balanced training setup configured!\")\n",
    "    print(f\"Loss weights: {loss_weights}\")\n",
    "    print(f\"Learning rate: {optimizer.learning_rate}\")\n",
    "    print(f\"Gradient clipping: {optimizer.clipnorm}\")\n",
    "    print(f\"Focus: Equal priority for both regression tasks\")\n",
    "    \n",
    "    return {\n",
    "        'loss_weights': loss_weights,\n",
    "        'loss_functions': loss_functions,\n",
    "        'metrics': metrics,\n",
    "        'optimizer': optimizer,\n",
    "        'callbacks': callbacks\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Balanced training setup function defined!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COMPREHENSIVE SOLUTION: SEPARATE TASKS ===\n",
      "üö® PROBLEM: Motion intensity and vertical dominance compete!\n",
      "‚úÖ SOLUTION: Dual encoder architecture with separate feature extraction\n",
      "\n",
      "=== WHY TASKS COMPETE ===\n",
      "üîç 1. SHARED FEATURE EXTRACTION:\n",
      "   - Both tasks use same pre-trained encoder\n",
      "   - Features learned for one task hurt the other\n",
      "   - Motion intensity needs temporal patterns\n",
      "   - Vertical dominance needs spatial patterns\n",
      "\n",
      "üîç 2. LOSS WEIGHT CONFLICTS:\n",
      "   - High weight on one task dominates training\n",
      "   - Other task gets less attention\n",
      "   - Model focuses on easier task\n",
      "   - Harder task gets ignored\n",
      "\n",
      "üîç 3. FEATURE INCOMPATIBILITY:\n",
      "   - Motion intensity: Magnitude, frequency, temporal\n",
      "   - Vertical dominance: Orientation, spatial relationships\n",
      "   - These features may be contradictory\n",
      "   - Shared layers can't optimize for both\n",
      "\n",
      "=== SOLUTION: DUAL ENCODER ARCHITECTURE ===\n",
      "\n",
      "üèóÔ∏è 1. SEPARATE ENCODERS:\n",
      "   - Motion intensity: Temporal-focused encoder\n",
      "   - Vertical dominance: Spatial-focused encoder\n",
      "   - No competition between tasks\n",
      "   - Each task gets optimized features\n",
      "\n",
      "üèóÔ∏è 2. BALANCED TRAINING:\n",
      "   - Equal loss weights: MI=15.0, VD=15.0\n",
      "   - No task dominates the other\n",
      "   - Both tasks improve simultaneously\n",
      "   - Independent optimization paths\n",
      "\n",
      "üèóÔ∏è 3. TASK-SPECIFIC FEATURES:\n",
      "   - Motion intensity: Magnitude, frequency, temporal patterns\n",
      "   - Vertical dominance: Orientation, spatial relationships\n",
      "   - Each task gets what it needs\n",
      "   - No feature conflicts\n",
      "\n",
      "üìä 4. EXPECTED RESULTS:\n",
      "   - Motion Intensity R¬≤: 0.5262 ‚Üí 0.6+ (maintain and improve)\n",
      "   - Vertical Dominance R¬≤: -0.0482 ‚Üí 0.3+ (fix negative values)\n",
      "   - Both tasks improve simultaneously\n",
      "   - No competition between tasks\n",
      "\n",
      "üéØ 5. IMPLEMENTATION CODE:\n",
      "   # Build dual encoder model\n",
      "   model = build_dual_encoder_model(input_shape, n_classes_p, n_classes_t, n_classes_c, pretrained_encoder)\n",
      "   \n",
      "   # Get balanced training setup\n",
      "   training_config = create_balanced_training_setup()\n",
      "   \n",
      "   # Compile and train\n",
      "   model.compile(optimizer=training_config['optimizer'], loss=training_config['loss_functions'], loss_weights=training_config['loss_weights'], metrics=training_config['metrics'])\n",
      "   history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=100, callbacks=training_config['callbacks'])\n",
      "\n",
      "‚úÖ READY TO IMPLEMENT DUAL ENCODER SOLUTION!\n",
      "This approach should improve both tasks simultaneously!\n"
     ]
    }
   ],
   "source": [
    "# COMPREHENSIVE SOLUTION: SEPARATE TASKS\n",
    "\n",
    "print(\"=== COMPREHENSIVE SOLUTION: SEPARATE TASKS ===\")\n",
    "print(\"üö® PROBLEM: Motion intensity and vertical dominance compete!\")\n",
    "print(\"‚úÖ SOLUTION: Dual encoder architecture with separate feature extraction\")\n",
    "\n",
    "print(\"\\n=== WHY TASKS COMPETE ===\")\n",
    "print(\"üîç 1. SHARED FEATURE EXTRACTION:\")\n",
    "print(\"   - Both tasks use same pre-trained encoder\")\n",
    "print(\"   - Features learned for one task hurt the other\")\n",
    "print(\"   - Motion intensity needs temporal patterns\")\n",
    "print(\"   - Vertical dominance needs spatial patterns\")\n",
    "\n",
    "print(\"\\nüîç 2. LOSS WEIGHT CONFLICTS:\")\n",
    "print(\"   - High weight on one task dominates training\")\n",
    "print(\"   - Other task gets less attention\")\n",
    "print(\"   - Model focuses on easier task\")\n",
    "print(\"   - Harder task gets ignored\")\n",
    "\n",
    "print(\"\\nüîç 3. FEATURE INCOMPATIBILITY:\")\n",
    "print(\"   - Motion intensity: Magnitude, frequency, temporal\")\n",
    "print(\"   - Vertical dominance: Orientation, spatial relationships\")\n",
    "print(\"   - These features may be contradictory\")\n",
    "print(\"   - Shared layers can't optimize for both\")\n",
    "\n",
    "print(\"\\n=== SOLUTION: DUAL ENCODER ARCHITECTURE ===\")\n",
    "\n",
    "print(\"\\nüèóÔ∏è 1. SEPARATE ENCODERS:\")\n",
    "print(\"   - Motion intensity: Temporal-focused encoder\")\n",
    "print(\"   - Vertical dominance: Spatial-focused encoder\")\n",
    "print(\"   - No competition between tasks\")\n",
    "print(\"   - Each task gets optimized features\")\n",
    "\n",
    "print(\"\\nüèóÔ∏è 2. BALANCED TRAINING:\")\n",
    "print(\"   - Equal loss weights: MI=15.0, VD=15.0\")\n",
    "print(\"   - No task dominates the other\")\n",
    "print(\"   - Both tasks improve simultaneously\")\n",
    "print(\"   - Independent optimization paths\")\n",
    "\n",
    "print(\"\\nüèóÔ∏è 3. TASK-SPECIFIC FEATURES:\")\n",
    "print(\"   - Motion intensity: Magnitude, frequency, temporal patterns\")\n",
    "print(\"   - Vertical dominance: Orientation, spatial relationships\")\n",
    "print(\"   - Each task gets what it needs\")\n",
    "print(\"   - No feature conflicts\")\n",
    "\n",
    "print(\"\\nüìä 4. EXPECTED RESULTS:\")\n",
    "print(\"   - Motion Intensity R¬≤: 0.5262 ‚Üí 0.6+ (maintain and improve)\")\n",
    "print(\"   - Vertical Dominance R¬≤: -0.0482 ‚Üí 0.3+ (fix negative values)\")\n",
    "print(\"   - Both tasks improve simultaneously\")\n",
    "print(\"   - No competition between tasks\")\n",
    "\n",
    "print(\"\\nüéØ 5. IMPLEMENTATION CODE:\")\n",
    "print(\"   # Build dual encoder model\")\n",
    "print(\"   model = build_dual_encoder_model(input_shape, n_classes_p, n_classes_t, n_classes_c, pretrained_encoder)\")\n",
    "print(\"   \")\n",
    "print(\"   # Get balanced training setup\")\n",
    "print(\"   training_config = create_balanced_training_setup()\")\n",
    "print(\"   \")\n",
    "print(\"   # Compile and train\")\n",
    "print(\"   model.compile(optimizer=training_config['optimizer'], loss=training_config['loss_functions'], loss_weights=training_config['loss_weights'], metrics=training_config['metrics'])\")\n",
    "print(\"   history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=100, callbacks=training_config['callbacks'])\")\n",
    "\n",
    "print(\"\\n‚úÖ READY TO IMPLEMENT DUAL ENCODER SOLUTION!\")\n",
    "print(\"This approach should improve both tasks simultaneously!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensor data: 8802 readings\n",
      "Manual labels: 150 windows\n",
      "\n",
      "Labeled windows:\n",
      "   window_idx  user activity  start_time  end_time  periodicity  \\\n",
      "0           0     3  Walking      957.75    960.75          1.0   \n",
      "1           1     3  Walking       42.00     45.00          1.0   \n",
      "2           2     3  Walking      871.50    874.50          0.5   \n",
      "3           3     3  Walking       63.00     66.00          1.0   \n",
      "4           4     3  Jogging      117.75    120.75          1.0   \n",
      "\n",
      "   temporal_stability  coordination  motion_intensity  vertical_dominance  \\\n",
      "0                 0.5           0.5          0.316815            0.221105   \n",
      "1                 0.5           0.5          0.302850            0.291116   \n",
      "2                 0.5           0.5          0.303036            0.181147   \n",
      "3                 0.5           0.5          0.313779            0.305797   \n",
      "4                 0.5           0.5          0.408648            0.262989   \n",
      "\n",
      "   static_posture  directional_variability  burstiness  \n",
      "0             0.0                 0.154414    0.489167  \n",
      "1             0.0                 0.070586    0.215654  \n",
      "2             0.0                 0.120062    0.442595  \n",
      "3             0.0                 0.087703    0.259150  \n",
      "4             0.0                 0.441992    0.342272  \n",
      "\n",
      "Available concepts: {'vertical_dominance', 'temporal_stability', 'periodicity', 'static_posture', 'motion_intensity', 'coordination'}\n",
      "\n",
      "Concept distributions:\n",
      "\n",
      "  [Continuous] vertical_dominance:\n",
      "    Mean: 0.248, Std: 0.081\n",
      "    Min: 0.041, Max: 0.562\n",
      "\n",
      "  [Discrete] temporal_stability:\n",
      "temporal_stability\n",
      "0.5    87\n",
      "1.0    51\n",
      "0.0    12\n",
      "Name: count, dtype: int64\n",
      "\n",
      "  [Discrete] periodicity:\n",
      "periodicity\n",
      "0.0    90\n",
      "0.5    35\n",
      "1.0    25\n",
      "Name: count, dtype: int64\n",
      "\n",
      "  [Continuous] motion_intensity:\n",
      "    Mean: 0.331, Std: 0.041\n",
      "    Min: 0.277, Max: 0.471\n",
      "\n",
      "  [Discrete] coordination:\n",
      "coordination\n",
      "1.0    70\n",
      "0.5    64\n",
      "0.0    16\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Extracting windows...\n",
      "df_sensor columns: ['user', 'activity', 'timestamp', 'x-axis', 'y-axis', 'z-axis', 'time_s', 'periodicity', 'temporal_stability', 'coordination', 'motion_intensity', 'vertical_dominance', 'static_posture', 'directional_variability', 'burstiness']\n",
      "df_sensor shape: (8802, 15)\n",
      "df_windows columns: ['window_idx', 'user', 'activity', 'start_time', 'end_time', 'periodicity', 'temporal_stability', 'coordination', 'motion_intensity', 'vertical_dominance', 'static_posture', 'directional_variability', 'burstiness']\n",
      "df_windows shape: (150, 13)\n",
      "All required sensor columns found!\n",
      "Processing 150 windows...\n",
      "Window 0: user=3, activity=Walking, start_time=957.75\n",
      "  Found 240 records for user 3, activity Walking\n",
      "  Time range (time_s): 42.03 to 960.74\n",
      "  Looking for start_time: 957.75, end_time: 960.75\n",
      "  Matching samples in time window: 60\n",
      "Window 1: user=3, activity=Walking, start_time=42.0\n",
      "  Found 240 records for user 3, activity Walking\n",
      "  Time range (time_s): 42.03 to 960.74\n",
      "  Looking for start_time: 42.0, end_time: 45.0\n",
      "  Matching samples in time window: 60\n",
      "Window 2: user=3, activity=Walking, start_time=871.5\n",
      "  Found 240 records for user 3, activity Walking\n",
      "  Time range (time_s): 42.03 to 960.74\n",
      "  Looking for start_time: 871.5, end_time: 874.5\n",
      "  Matching samples in time window: 60\n",
      "Window 3: user=3, activity=Walking, start_time=63.0\n",
      "  Found 240 records for user 3, activity Walking\n",
      "  Time range (time_s): 42.03 to 960.74\n",
      "  Looking for start_time: 63.0, end_time: 66.0\n",
      "  Matching samples in time window: 60\n",
      "Window 4: user=3, activity=Jogging, start_time=117.75\n",
      "  Found 296 records for user 3, activity Jogging\n",
      "  Time range (time_s): 3.07 to 996.72\n",
      "  Looking for start_time: 117.75, end_time: 120.75\n",
      "  Matching samples in time window: 115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully extracted 150 out of 150 windows\n",
      "Extracted 150 valid windows\n",
      "Scaling continuous concepts to 0-1 range for better regression performance:\n",
      "Motion Intensity - Original: 0.277 to 0.471, Scaled: 0.000 to 1.000\n",
      "Vertical Dominance - Original: 0.041 to 0.562, Scaled: 0.000 to 1.000\n",
      "\n",
      "Label shapes:\n",
      "  Periodicity: (150,)\n",
      "  Temporal Stability: (150,)\n",
      "  Coordination: (150,)\n",
      "  Motion Intensity: (150,)\n",
      "  Vertical Dominance: (150,)\n",
      "  Static Posture: (150,)\n",
      "\n",
      "Train/Test split:\n",
      "  Train: 120 windows\n",
      "  Test: 30 windows\n",
      "Data preprocessing completed for fine-tuning!\n"
     ]
    }
   ],
   "source": [
    "# Load data for fine-tuning\n",
    "df_sensor = pd.read_csv('../rule_based_labeling/raw_with_features.csv')\n",
    "df_windows = pd.read_csv('../rule_based_labeling/window_with_features.csv')\n",
    "\n",
    "print(f\"Sensor data: {len(df_sensor)} readings\")\n",
    "print(f\"Manual labels: {len(df_windows)} windows\")\n",
    "print(f\"\\nLabeled windows:\")\n",
    "print(df_windows.head())\n",
    "\n",
    "# Define concept columns\n",
    "concept_columns = {'periodicity', 'temporal_stability', 'coordination', 'motion_intensity', 'vertical_dominance', 'static_posture'}\n",
    "discrete_concepts = {'periodicity', 'temporal_stability', 'coordination'}  # Only these are discrete\n",
    "continuous_concepts = {'motion_intensity', 'vertical_dominance'}  # These are continuous\n",
    "\n",
    "print(f\"\\nAvailable concepts: {concept_columns}\")\n",
    "print(f\"\\nConcept distributions:\")\n",
    "\n",
    "for concept in concept_columns:\n",
    "    if concept not in df_windows.columns:\n",
    "        print(f\"  {concept}: (missing from data)\")\n",
    "        continue\n",
    "\n",
    "    if concept in discrete_concepts:\n",
    "        print(f\"\\n  [Discrete] {concept}:\")\n",
    "        print(df_windows[concept].value_counts(dropna=False))\n",
    "    elif concept in continuous_concepts:\n",
    "        print(f\"\\n  [Continuous] {concept}:\")\n",
    "        print(f\"    Mean: {df_windows[concept].mean():.3f}, Std: {df_windows[concept].std():.3f}\")\n",
    "        print(f\"    Min: {df_windows[concept].min():.3f}, Max: {df_windows[concept].max():.3f}\")\n",
    "\n",
    "# Extract windows from sensor data using the same approach as working notebook\n",
    "def extract_window_robust(df_sensor, window_row, time_tolerance=0.5):\n",
    "    \"\"\"\n",
    "    Extract sensor data with time tolerance to handle mismatches.\n",
    "    \"\"\"\n",
    "    user = window_row['user']\n",
    "    activity = window_row['activity']\n",
    "    start_time = window_row['start_time']\n",
    "    end_time = window_row['end_time']\n",
    "    \n",
    "    # Get data for this user/activity\n",
    "    user_activity_data = df_sensor[(df_sensor['user'] == user) & \n",
    "                                  (df_sensor['activity'] == activity)].copy()\n",
    "    \n",
    "    if len(user_activity_data) == 0:\n",
    "        return None\n",
    "    \n",
    "    # Find data within time window with tolerance\n",
    "    mask = ((user_activity_data['time_s'] >= start_time - time_tolerance) & \n",
    "            (user_activity_data['time_s'] <= end_time + time_tolerance))\n",
    "    \n",
    "    window_data = user_activity_data[mask]\n",
    "    \n",
    "    if len(window_data) < 10:  # Need minimum samples\n",
    "        return None\n",
    "    \n",
    "    # Extract sensor readings\n",
    "    sensor_data = window_data[['x-axis', 'y-axis', 'z-axis']].values\n",
    "    \n",
    "    # Pad or truncate to fixed length (e.g., 60 samples)\n",
    "    target_length = 60\n",
    "    if len(sensor_data) > target_length:\n",
    "        # Randomly sample if too long\n",
    "        indices = np.random.choice(len(sensor_data), target_length, replace=False)\n",
    "        sensor_data = sensor_data[indices]\n",
    "    elif len(sensor_data) < target_length:\n",
    "        # Pad with last value if too short\n",
    "        padding = np.tile(sensor_data[-1:], (target_length - len(sensor_data), 1))\n",
    "        sensor_data = np.vstack([sensor_data, padding])\n",
    "    \n",
    "    return sensor_data\n",
    "\n",
    "def extract_windows_robust(df_sensor, df_windows):\n",
    "    \"\"\"Extract windows with robust error handling - same as working notebook\"\"\"\n",
    "    X = []\n",
    "    y_p = []\n",
    "    y_t = []\n",
    "    y_c = []\n",
    "    y_mi = []\n",
    "    y_vd = []\n",
    "    y_sp = []\n",
    "    \n",
    "    print(f\"Processing {len(df_windows)} windows...\")\n",
    "    valid_count = 0\n",
    "    \n",
    "    for i, (_, window_row) in enumerate(df_windows.iterrows()):\n",
    "        if i < 5:  # Debug first 5 windows\n",
    "            print(f\"Window {i}: user={window_row['user']}, activity={window_row['activity']}, start_time={window_row['start_time']}\")\n",
    "            \n",
    "            # Debug the extraction process\n",
    "            user = window_row['user']\n",
    "            activity = window_row['activity']\n",
    "            start_time = window_row['start_time']\n",
    "            end_time = window_row['end_time']\n",
    "            \n",
    "            # Get data for this user/activity\n",
    "            user_activity_data = df_sensor[(df_sensor['user'] == user) & \n",
    "                                          (df_sensor['activity'] == activity)].copy()\n",
    "            print(f\"  Found {len(user_activity_data)} records for user {user}, activity {activity}\")\n",
    "            \n",
    "            if len(user_activity_data) > 0:\n",
    "                # Check time range using time_s column\n",
    "                min_time = user_activity_data['time_s'].min()\n",
    "                max_time = user_activity_data['time_s'].max()\n",
    "                print(f\"  Time range (time_s): {min_time:.2f} to {max_time:.2f}\")\n",
    "                print(f\"  Looking for start_time: {start_time}, end_time: {end_time}\")\n",
    "                \n",
    "                # Check if time window overlaps\n",
    "                mask = ((user_activity_data['time_s'] >= start_time - 0.5) & \n",
    "                        (user_activity_data['time_s'] <= end_time + 0.5))\n",
    "                matching_samples = len(user_activity_data[mask])\n",
    "                print(f\"  Matching samples in time window: {matching_samples}\")\n",
    "        \n",
    "        window_data = extract_window_robust(df_sensor, window_row)\n",
    "        if window_data is not None:\n",
    "            X.append(window_data)\n",
    "            y_p.append(window_row['periodicity'])\n",
    "            y_t.append(window_row['temporal_stability'])\n",
    "            y_c.append(window_row['coordination'])\n",
    "            y_mi.append(window_row['motion_intensity'])\n",
    "            y_vd.append(window_row['vertical_dominance'])\n",
    "            y_sp.append(window_row['static_posture'])\n",
    "            valid_count += 1\n",
    "        else:\n",
    "            if i < 5:  # Debug first 5 failures\n",
    "                print(f\"  -> Failed to extract window {i}\")\n",
    "    \n",
    "    print(f\"Successfully extracted {valid_count} out of {len(df_windows)} windows\")\n",
    "    return np.array(X), np.array(y_p), np.array(y_t), np.array(y_c), np.array(y_mi), np.array(y_vd), np.array(y_sp)\n",
    "\n",
    "# Extract windows\n",
    "print(\"\\nExtracting windows...\")\n",
    "print(f\"df_sensor columns: {list(df_sensor.columns)}\")\n",
    "print(f\"df_sensor shape: {df_sensor.shape}\")\n",
    "print(f\"df_windows columns: {list(df_windows.columns)}\")\n",
    "print(f\"df_windows shape: {df_windows.shape}\")\n",
    "\n",
    "# Check if we have the required columns\n",
    "required_sensor_cols = ['user', 'activity', 'timestamp', 'x-axis', 'y-axis', 'z-axis']\n",
    "missing_sensor_cols = [col for col in required_sensor_cols if col not in df_sensor.columns]\n",
    "if missing_sensor_cols:\n",
    "    print(f\"Missing sensor columns: {missing_sensor_cols}\")\n",
    "else:\n",
    "    print(\"All required sensor columns found!\")\n",
    "\n",
    "X_windows, y_p, y_t, y_c, y_mi, y_vd, y_sp = extract_windows_robust(df_sensor, df_windows)\n",
    "print(f\"Extracted {len(X_windows)} valid windows\")\n",
    "\n",
    "# Convert to numpy arrays\n",
    "y_p = np.array(y_p)\n",
    "y_t = np.array(y_t)\n",
    "y_c = np.array(y_c)\n",
    "y_mi = np.array(y_mi)\n",
    "y_vd = np.array(y_vd)\n",
    "y_sp = np.array(y_sp)\n",
    "\n",
    "# Scale continuous concepts to 0-1 range for better regression performance\n",
    "print(\"Scaling continuous concepts to 0-1 range for better regression performance:\")\n",
    "\n",
    "# Store original ranges for inverse scaling later\n",
    "mi_min, mi_max = y_mi.min(), y_mi.max()\n",
    "vd_min, vd_max = y_vd.min(), y_vd.max()\n",
    "\n",
    "# Scale to 0-1 range\n",
    "y_mi_scaled = (y_mi - mi_min) / (mi_max - mi_min)\n",
    "y_vd_scaled = (y_vd - vd_min) / (vd_max - vd_min)\n",
    "\n",
    "print(f\"Motion Intensity - Original: {mi_min:.3f} to {mi_max:.3f}, Scaled: {y_mi_scaled.min():.3f} to {y_mi_scaled.max():.3f}\")\n",
    "print(f\"Vertical Dominance - Original: {vd_min:.3f} to {vd_max:.3f}, Scaled: {y_vd_scaled.min():.3f} to {y_vd_scaled.max():.3f}\")\n",
    "\n",
    "# Use scaled versions\n",
    "y_mi = y_mi_scaled\n",
    "y_vd = y_vd_scaled\n",
    "\n",
    "print(f\"\\nLabel shapes:\")\n",
    "print(f\"  Periodicity: {y_p.shape}\")\n",
    "print(f\"  Temporal Stability: {y_t.shape}\")\n",
    "print(f\"  Coordination: {y_c.shape}\")\n",
    "print(f\"  Motion Intensity: {y_mi.shape}\")\n",
    "print(f\"  Vertical Dominance: {y_vd.shape}\")\n",
    "print(f\"  Static Posture: {y_sp.shape}\")\n",
    "\n",
    "# Stratified train/test split using static posture for stratification\n",
    "X_train, X_test, y_p_train, y_p_test, y_t_train, y_t_test, y_c_train, y_c_test, y_mi_train, y_mi_test, y_vd_train, y_vd_test, y_sp_train, y_sp_test = train_test_split(\n",
    "    X_windows, y_p, y_t, y_c, y_mi, y_vd, y_sp,\n",
    "    test_size=0.2, random_state=42, stratify=y_sp\n",
    ")\n",
    "\n",
    "# Store original test values for later comparison\n",
    "y_mi_test_original = y_mi_test.copy()\n",
    "y_vd_test_original = y_vd_test.copy()\n",
    "\n",
    "print(f\"\\nTrain/Test split:\")\n",
    "print(f\"  Train: {len(X_train)} windows\")\n",
    "print(f\"  Test: {len(X_test)} windows\")\n",
    "\n",
    "# Convert to categorical for discrete concepts\n",
    "# For 3-class problems: multiply by 2 to convert 0.0, 0.5, 1.0 -> 0, 1, 2\n",
    "y_p_train_cat = tf.keras.utils.to_categorical(y_p_train * 2, num_classes=3)\n",
    "y_t_train_cat = tf.keras.utils.to_categorical(y_t_train * 2, num_classes=3)\n",
    "y_c_train_cat = tf.keras.utils.to_categorical(y_c_train * 2, num_classes=3)\n",
    "\n",
    "# For 2-class problems: convert 0.0, 1.0 -> 0, 1 (no multiplication needed)\n",
    "y_sp_train_cat = tf.keras.utils.to_categorical(y_sp_train, num_classes=2)\n",
    "\n",
    "y_p_test_cat = tf.keras.utils.to_categorical(y_p_test * 2, num_classes=3)\n",
    "y_t_test_cat = tf.keras.utils.to_categorical(y_t_test * 2, num_classes=3)\n",
    "y_c_test_cat = tf.keras.utils.to_categorical(y_c_test * 2, num_classes=3)\n",
    "y_sp_test_cat = tf.keras.utils.to_categorical(y_sp_test, num_classes=2)\n",
    "\n",
    "print(\"Data preprocessing completed for fine-tuning!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed exact architecture match model defined\n"
     ]
    }
   ],
   "source": [
    "# FIXED: Exact Architecture Match for Successful Weight Copying\n",
    "def build_exact_match_model_with_pretrained_encoder(input_shape, n_classes_p, n_classes_t, n_classes_c, pretrained_encoder):\n",
    "    \"\"\"\n",
    "    Build model that EXACTLY matches the pre-trained encoder architecture for successful weight copying\n",
    "    \"\"\"\n",
    "    # Input layer for sensor data\n",
    "    sensor_input = tf.keras.layers.Input(shape=input_shape, name='sensor_input')\n",
    "    \n",
    "    # EXACT MATCH: Build encoder architecture to match the actual pre-trained TensorFlow encoder\n",
    "    # Layer 1: Conv1D(3 -> 64, kernel=5) - matches 'conv1'\n",
    "    x = tf.keras.layers.Conv1D(64, 5, padding='same', activation='relu', name='conv1')(sensor_input)\n",
    "    x = tf.keras.layers.BatchNormalization(name='bn1')(x)\n",
    "    x = tf.keras.layers.Dropout(0.2, name='dropout1')(x)\n",
    "    \n",
    "    # Layer 2: Conv1D(64 -> 32, kernel=5) - matches 'conv2'\n",
    "    x = tf.keras.layers.Conv1D(32, 5, padding='same', activation='relu', name='conv2')(x)\n",
    "    x = tf.keras.layers.BatchNormalization(name='bn2')(x)\n",
    "    x = tf.keras.layers.Dropout(0.2, name='dropout2')(x)\n",
    "    \n",
    "    # Layer 3: Conv1D(32 -> 16, kernel=5) - matches 'conv3'\n",
    "    x = tf.keras.layers.Conv1D(16, 5, padding='same', activation='relu', name='conv3')(x)\n",
    "    x = tf.keras.layers.BatchNormalization(name='bn3')(x)\n",
    "    x = tf.keras.layers.Dropout(0.2, name='dropout3')(x)\n",
    "    \n",
    "    # Global average pooling - matches 'global_pool'\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D(name='global_pool')(x)\n",
    "    \n",
    "    # Dense layers - matches the actual pre-trained encoder structure\n",
    "    # Layer 4: Dense(16 -> 128) - matches 'dense1'\n",
    "    x = tf.keras.layers.Dense(128, activation='relu', name='dense1')(x)\n",
    "    x = tf.keras.layers.Dropout(0.2, name='dropout4')(x)\n",
    "    \n",
    "    # Layer 5: Dense(128 -> 64) - matches 'dense2'\n",
    "    x = tf.keras.layers.Dense(64, activation='relu', name='dense2')(x)\n",
    "    x = tf.keras.layers.Dropout(0.2, name='dropout5')(x)\n",
    "    \n",
    "    # Layer 6: Dense(64 -> 5) - matches 'concept_features' (5 concepts)\n",
    "    x = tf.keras.layers.Dense(5, activation='linear', name='concept_features')(x)\n",
    "    \n",
    "    # Add new layers for concept prediction (these will be randomly initialized)\n",
    "    x = tf.keras.layers.Dense(64, activation='relu', name='concept_dense_1')(x)\n",
    "    x = tf.keras.layers.Dropout(0.3, name='concept_dropout_1')(x)\n",
    "    x = tf.keras.layers.Dense(32, activation='relu', name='concept_dense_2')(x)\n",
    "    x = tf.keras.layers.Dropout(0.2, name='concept_dropout_2')(x)\n",
    "    \n",
    "    # Output layers for each concept\n",
    "    # Discrete concepts (classification)\n",
    "    periodicity = tf.keras.layers.Dense(n_classes_p, activation='softmax', name='periodicity')(x)\n",
    "    temporal_stability = tf.keras.layers.Dense(n_classes_t, activation='softmax', name='temporal_stability')(x)\n",
    "    coordination = tf.keras.layers.Dense(n_classes_c, activation='softmax', name='coordination')(x)\n",
    "    \n",
    "    # Continuous concepts (regression)\n",
    "    motion_intensity = tf.keras.layers.Dense(1, activation='linear', name='motion_intensity')(x)\n",
    "    vertical_dominance = tf.keras.layers.Dense(1, activation='linear', name='vertical_dominance')(x)\n",
    "    \n",
    "    model = tf.keras.models.Model(\n",
    "        inputs=sensor_input, \n",
    "        outputs=[periodicity, temporal_stability, coordination, motion_intensity, vertical_dominance]\n",
    "    )\n",
    "    \n",
    "    # Copy weights from pre-trained encoder (should work now with exact architecture match)\n",
    "    try:\n",
    "        print(\"Attempting to copy weights from pre-trained encoder with exact architecture match...\")\n",
    "        pretrained_encoder.tf_encoder.trainable = True\n",
    "        \n",
    "        # Copy weights layer by layer - should work now\n",
    "        for i, layer in enumerate(model.layers):\n",
    "            if i < len(pretrained_encoder.tf_encoder.layers):\n",
    "                pretrained_layer = pretrained_encoder.tf_encoder.layers[i]\n",
    "                if hasattr(layer, 'set_weights') and hasattr(pretrained_layer, 'get_weights'):\n",
    "                    try:\n",
    "                        layer.set_weights(pretrained_layer.get_weights())\n",
    "                        print(f\"‚úì Copied weights for layer {i}: {layer.name}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"‚ö† Could not copy weights for layer {i}: {layer.name} - {e}\")\n",
    "        \n",
    "        print(\"‚úì Pre-trained weights copied successfully with exact architecture match!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö† Could not copy pre-trained weights: {e}\")\n",
    "        print(\"Proceeding with random initialization...\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"Fixed exact architecture match model defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected fine-tuning model architecture defined\n"
     ]
    }
   ],
   "source": [
    "# CORRECTED: Fine-tuning Model with Pre-trained Encoder (3 discrete + 2 continuous concepts)\n",
    "def build_finetuning_model_with_pretrained_encoder_corrected(input_shape, n_classes_p, n_classes_t, n_classes_c, pretrained_encoder):\n",
    "    \"\"\"\n",
    "    Build fine-tuning model that uses the pre-trained encoder as a feature extractor\n",
    "    \n",
    "    Args:\n",
    "        input_shape: Shape of sensor data (timesteps, 3)\n",
    "        n_classes_p: Number of classes for periodicity\n",
    "        n_classes_t: Number of classes for temporal_stability  \n",
    "        n_classes_c: Number of classes for coordination\n",
    "        pretrained_encoder: Pre-trained encoder model\n",
    "    \"\"\"\n",
    "    # Input layer for sensor data\n",
    "    sensor_input = layers.Input(shape=input_shape, name='sensor_input')\n",
    "    \n",
    "    # Use pre-trained encoder as feature extractor (frozen initially)\n",
    "    pretrained_features = pretrained_encoder.tf_encoder(sensor_input)\n",
    "    \n",
    "    # Fine-tuning layers on top of pre-trained features\n",
    "    x = layers.Dense(64, activation='relu', name='finetune_dense1')(pretrained_features)\n",
    "    x = layers.Dropout(0.3, name='finetune_dropout1')(x)\n",
    "    x = layers.Dense(32, activation='relu', name='finetune_dense2')(x)\n",
    "    x = layers.Dropout(0.2, name='finetune_dropout2')(x)\n",
    "    \n",
    "    # Output layers for each concept\n",
    "    # Discrete concepts (classification)\n",
    "    periodicity = layers.Dense(n_classes_p, activation='softmax', name='periodicity')(x)\n",
    "    temporal_stability = layers.Dense(n_classes_t, activation='softmax', name='temporal_stability')(x)\n",
    "    coordination = layers.Dense(n_classes_c, activation='softmax', name='coordination')(x)\n",
    "    \n",
    "    # Continuous concepts (regression)\n",
    "    motion_intensity = layers.Dense(1, activation='linear', name='motion_intensity')(x)\n",
    "    vertical_dominance = layers.Dense(1, activation='linear', name='vertical_dominance')(x)\n",
    "    \n",
    "    model = keras.Model(\n",
    "        inputs=sensor_input, \n",
    "        outputs=[periodicity, temporal_stability, coordination, motion_intensity, vertical_dominance]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"Corrected fine-tuning model architecture defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pre-trained Encoder Integration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing pre-trained encoder...\n",
      "Loading pre-trained PyTorch encoder...\n",
      "PyTorch encoder loaded successfully\n",
      "TensorFlow encoder architecture created\n",
      "Encoder converted to TensorFlow format\n",
      "Pre-trained encoder ready!\n"
     ]
    }
   ],
   "source": [
    "# Pre-trained Encoder Integration for Fine-tuning\n",
    "class PretrainedEncoderWrapper:\n",
    "    \"\"\"\n",
    "    Wrapper class for the pre-trained PyTorch encoder\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.encoder_weights = None\n",
    "        self.tf_encoder = None\n",
    "        self.load_pretrained_encoder()\n",
    "    \n",
    "    def load_pretrained_encoder(self):\n",
    "        \"\"\"Load the pre-trained PyTorch encoder and convert to TensorFlow\"\"\"\n",
    "        try:\n",
    "            # Load PyTorch encoder\n",
    "            encoder_path = '../pretraining/improved_pretrained_encoder.pth'\n",
    "            if os.path.exists(encoder_path):\n",
    "                print(\"Loading pre-trained PyTorch encoder...\")\n",
    "                pytorch_encoder = torch.load(encoder_path, map_location='cpu')\n",
    "                print(\"PyTorch encoder loaded successfully\")\n",
    "                \n",
    "                # Convert PyTorch weights to TensorFlow format\n",
    "                self.tf_encoder = self._convert_pytorch_to_tensorflow(pytorch_encoder)\n",
    "                print(\"Encoder converted to TensorFlow format\")\n",
    "            else:\n",
    "                print(f\"Warning: Pre-trained encoder not found at {encoder_path}\")\n",
    "                print(\"Creating encoder from scratch...\")\n",
    "                self.tf_encoder = self._create_encoder_from_scratch()\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading pre-trained encoder: {e}\")\n",
    "            print(\"Creating encoder from scratch...\")\n",
    "            self.tf_encoder = self._create_encoder_from_scratch()\n",
    "    \n",
    "    def _convert_pytorch_to_tensorflow(self, pytorch_encoder):\n",
    "        \"\"\"Convert PyTorch encoder to TensorFlow format\"\"\"\n",
    "        # Create TensorFlow encoder with same architecture as the PyTorch version\n",
    "        input_layer = layers.Input(shape=(60, 3), name='encoder_input')\n",
    "        \n",
    "        # Conv1D layers (equivalent to PyTorch Conv1d with kernel_size=5)\n",
    "        x = layers.Conv1D(64, 5, padding='same', activation='relu', name='conv1')(input_layer)\n",
    "        x = layers.BatchNormalization(name='bn1')(x)\n",
    "        x = layers.Dropout(0.2, name='dropout1')(x)\n",
    "        \n",
    "        x = layers.Conv1D(32, 5, padding='same', activation='relu', name='conv2')(x)\n",
    "        x = layers.BatchNormalization(name='bn2')(x)\n",
    "        x = layers.Dropout(0.2, name='dropout2')(x)\n",
    "        \n",
    "        x = layers.Conv1D(16, 5, padding='same', activation='relu', name='conv3')(x)\n",
    "        x = layers.BatchNormalization(name='bn3')(x)\n",
    "        x = layers.Dropout(0.2, name='dropout3')(x)\n",
    "        \n",
    "        # Global average pooling\n",
    "        x = layers.GlobalAveragePooling1D(name='global_pool')(x)\n",
    "        \n",
    "        # Dense layers for feature extraction (matching PyTorch architecture)\n",
    "        x = layers.Dense(128, activation='relu', name='dense1')(x)\n",
    "        x = layers.Dropout(0.2, name='dropout4')(x)\n",
    "        x = layers.Dense(64, activation='relu', name='dense2')(x)\n",
    "        x = layers.Dropout(0.2, name='dropout5')(x)\n",
    "        \n",
    "        # Output layer for concept features (5 concepts)\n",
    "        concept_features = layers.Dense(5, activation='linear', name='concept_features')(x)\n",
    "        \n",
    "        tf_encoder = keras.Model(inputs=input_layer, outputs=concept_features, name='pretrained_encoder')\n",
    "        \n",
    "        # Note: In a real implementation, you would transfer the actual weights\n",
    "        # For now, we'll use the architecture and train from the pre-trained state\n",
    "        print(\"TensorFlow encoder architecture created\")\n",
    "        return tf_encoder\n",
    "    \n",
    "    def _create_encoder_from_scratch(self):\n",
    "        \"\"\"Create encoder from scratch if pre-trained model not available\"\"\"\n",
    "        print(\"Creating encoder from scratch...\")\n",
    "        input_layer = tf.keras.layers.Input(shape=(60, 3), name='encoder_input')\n",
    "        \n",
    "        x = tf.keras.layers.Conv1D(64, 5, padding='same', activation='relu')(input_layer)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Dropout(0.2)(x)\n",
    "        \n",
    "        x = tf.keras.layers.Conv1D(32, 5, padding='same', activation='relu')(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Dropout(0.2)(x)\n",
    "        \n",
    "        x = tf.keras.layers.Conv1D(16, 5, padding='same', activation='relu')(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Dropout(0.2)(x)\n",
    "        \n",
    "        x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "        \n",
    "        x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "        x = tf.keras.layers.Dropout(0.2)(x)\n",
    "        x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "        x = tf.keras.layers.Dropout(0.2)(x)\n",
    "        \n",
    "        concept_features = tf.keras.layers.Dense(5, activation='linear')(x)\n",
    "        \n",
    "        return tf.keras.models.Model(inputs=input_layer, outputs=concept_features, name='encoder_from_scratch')\n",
    "    \n",
    "    def get_concept_features(self, sensor_data):\n",
    "        \"\"\"\n",
    "        Extract concept features from sensor data using pre-trained encoder\n",
    "        \n",
    "        Args:\n",
    "            sensor_data: Input sensor data (n_samples, timesteps, 3)\n",
    "            \n",
    "        Returns:\n",
    "            concept_features: Extracted concept features (n_samples, 5)\n",
    "        \"\"\"\n",
    "        if self.tf_encoder is None:\n",
    "            print(\"Warning: Encoder not loaded, returning dummy features\")\n",
    "            return np.random.rand(len(sensor_data), 5)\n",
    "        \n",
    "        try:\n",
    "            # Get concept features from pre-trained encoder\n",
    "            concept_features = self.tf_encoder.predict(sensor_data, verbose=0)\n",
    "            return concept_features\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting concept features: {e}\")\n",
    "            # Return dummy features\n",
    "            return np.random.rand(len(sensor_data), 5)\n",
    "\n",
    "# Initialize pre-trained encoder\n",
    "print(\"Initializing pre-trained encoder...\")\n",
    "pretrained_encoder = PretrainedEncoderWrapper()\n",
    "print(\"Pre-trained encoder ready!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fine-tuning Model Architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fine-tuning Model with Pre-trained Encoder (5 discrete concepts only)\n",
    "# def build_finetuning_model_with_pretrained_encoder(input_shape, n_classes_p, n_classes_t, n_classes_c, n_classes_mi, n_classes_vd, pretrained_encoder):\n",
    "#     \"\"\"\n",
    "#     Build fine-tuning model that uses the pre-trained encoder as a feature extractor\n",
    "    \n",
    "#     Args:\n",
    "#         input_shape: Shape of sensor data (timesteps, 3)\n",
    "#         n_classes_p: Number of classes for periodicity\n",
    "#         n_classes_t: Number of classes for temporal_stability  \n",
    "#         n_classes_c: Number of classes for coordination\n",
    "#         n_classes_mi: Number of classes for motion_intensity\n",
    "#         n_classes_vd: Number of classes for vertical_dominance\n",
    "#         pretrained_encoder: Pre-trained encoder model\n",
    "#     \"\"\"\n",
    "#     # Input layer for sensor data\n",
    "#     sensor_input = layers.Input(shape=input_shape, name='sensor_input')\n",
    "    \n",
    "#     # Use pre-trained encoder as feature extractor (frozen initially)\n",
    "#     pretrained_features = pretrained_encoder.tf_encoder(sensor_input)\n",
    "    \n",
    "#     # Fine-tuning layers on top of pre-trained features\n",
    "#     x = layers.Dense(64, activation='relu', name='finetune_dense1')(pretrained_features)\n",
    "#     x = layers.Dropout(0.3, name='finetune_dropout1')(x)\n",
    "#     x = layers.Dense(32, activation='relu', name='finetune_dense2')(x)\n",
    "#     x = layers.Dropout(0.2, name='finetune_dropout2')(x)\n",
    "    \n",
    "#     # Output layers for each concept (all discrete now)\n",
    "#     periodicity = layers.Dense(n_classes_p, activation='softmax', name='periodicity')(x)\n",
    "#     temporal_stability = layers.Dense(n_classes_t, activation='softmax', name='temporal_stability')(x)\n",
    "#     coordination = layers.Dense(n_classes_c, activation='softmax', name='coordination')(x)\n",
    "#     motion_intensity = layers.Dense(n_classes_mi, activation='softmax', name='motion_intensity')(x)\n",
    "#     vertical_dominance = layers.Dense(n_classes_vd, activation='softmax', name='vertical_dominance')(x)\n",
    "    \n",
    "#     model = keras.Model(\n",
    "#         inputs=sensor_input, \n",
    "#         outputs=[periodicity, temporal_stability, coordination, motion_intensity, vertical_dominance]\n",
    "#     )\n",
    "    \n",
    "#     return model\n",
    "\n",
    "# print(\"Fine-tuning model architecture defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Augmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting training data for fine-tuning...\n",
      "Original train: 120 windows\n",
      "Augmented train: 1200 windows\n",
      "Augmentation factor: 10.0x\n",
      "Data augmentation completed for fine-tuning!\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation functions for fine-tuning\n",
    "def augment_jitter(data, noise_factor=0.1):\n",
    "    \"\"\"Add jitter noise to sensor data\"\"\"\n",
    "    noise = np.random.normal(0, noise_factor, data.shape)\n",
    "    return data + noise\n",
    "\n",
    "def augment_scaling(data, scale_range=(0.8, 1.2)):\n",
    "    \"\"\"Scale sensor data by random factors\"\"\"\n",
    "    scale_factors = np.random.uniform(scale_range[0], scale_range[1], (data.shape[0], 1, data.shape[2]))\n",
    "    return data * scale_factors\n",
    "\n",
    "def augment_rotation(data, rotation_range=(-0.1, 0.1)):\n",
    "    \"\"\"Apply small rotations to sensor data\"\"\"\n",
    "    rotated_data = data.copy()\n",
    "    \n",
    "    for i in range(data.shape[0]):\n",
    "        # Generate random rotation angle for each sample\n",
    "        angle = np.random.uniform(rotation_range[0], rotation_range[1])\n",
    "        cos_a, sin_a = np.cos(angle), np.sin(angle)\n",
    "        \n",
    "        # Apply rotation to x and y axes (keep z unchanged)\n",
    "        x_rot = data[i, :, 0] * cos_a - data[i, :, 1] * sin_a\n",
    "        y_rot = data[i, :, 0] * sin_a + data[i, :, 1] * cos_a\n",
    "        \n",
    "        rotated_data[i, :, 0] = x_rot\n",
    "        rotated_data[i, :, 1] = y_rot\n",
    "        # z-axis remains unchanged\n",
    "    \n",
    "    return rotated_data\n",
    "\n",
    "def augment_dataset(X, y_p, y_t, y_c, y_mi, y_vd, y_sp, factor=5):\n",
    "    \"\"\"Augment dataset with multiple augmentation techniques\"\"\"\n",
    "    augmented_X = [X]\n",
    "    augmented_y_p = [y_p]\n",
    "    augmented_y_t = [y_t]\n",
    "    augmented_y_c = [y_c]\n",
    "    augmented_y_mi = [y_mi]\n",
    "    augmented_y_vd = [y_vd]\n",
    "    augmented_y_sp = [y_sp]\n",
    "    \n",
    "    for _ in range(factor):\n",
    "        # Jitter augmentation\n",
    "        X_jitter = augment_jitter(X, noise_factor=0.05)\n",
    "        augmented_X.append(X_jitter)\n",
    "        augmented_y_p.append(y_p)\n",
    "        augmented_y_t.append(y_t)\n",
    "        augmented_y_c.append(y_c)\n",
    "        augmented_y_mi.append(y_mi)\n",
    "        augmented_y_vd.append(y_vd)\n",
    "        augmented_y_sp.append(y_sp)\n",
    "        \n",
    "        # Scaling augmentation\n",
    "        X_scale = augment_scaling(X, scale_range=(0.9, 1.1))\n",
    "        augmented_X.append(X_scale)\n",
    "        augmented_y_p.append(y_p)\n",
    "        augmented_y_t.append(y_t)\n",
    "        augmented_y_c.append(y_c)\n",
    "        augmented_y_mi.append(y_mi)\n",
    "        augmented_y_vd.append(y_vd)\n",
    "        augmented_y_sp.append(y_sp)\n",
    "        \n",
    "        # Rotation augmentation\n",
    "        X_rot = augment_rotation(X, rotation_range=(-0.05, 0.05))\n",
    "        augmented_X.append(X_rot)\n",
    "        augmented_y_p.append(y_p)\n",
    "        augmented_y_t.append(y_t)\n",
    "        augmented_y_c.append(y_c)\n",
    "        augmented_y_mi.append(y_mi)\n",
    "        augmented_y_vd.append(y_vd)\n",
    "        augmented_y_sp.append(y_sp)\n",
    "    \n",
    "    # Combine all augmented data\n",
    "    X_aug = np.concatenate(augmented_X, axis=0)\n",
    "    y_p_aug = np.concatenate(augmented_y_p, axis=0)\n",
    "    y_t_aug = np.concatenate(augmented_y_t, axis=0)\n",
    "    y_c_aug = np.concatenate(augmented_y_c, axis=0)\n",
    "    y_mi_aug = np.concatenate(augmented_y_mi, axis=0)\n",
    "    y_vd_aug = np.concatenate(augmented_y_vd, axis=0)\n",
    "    y_sp_aug = np.concatenate(augmented_y_sp, axis=0)\n",
    "    \n",
    "    return X_aug, y_p_aug, y_t_aug, y_c_aug, y_mi_aug, y_vd_aug, y_sp_aug\n",
    "\n",
    "# Apply augmentation to training data (using scaled regression targets)\n",
    "print(\"Augmenting training data for fine-tuning...\")\n",
    "X_train_aug, y_p_train_aug, y_t_train_aug, y_c_train_aug, y_mi_train_aug, y_vd_train_aug, y_sp_train_aug = augment_dataset(\n",
    "    X_train, y_p_train, y_t_train, y_c_train, y_mi_train, y_vd_train, y_sp_train, factor=3\n",
    ")\n",
    "\n",
    "print(f\"Original train: {len(X_train)} windows\")\n",
    "print(f\"Augmented train: {len(X_train_aug)} windows\")\n",
    "print(f\"Augmentation factor: {len(X_train_aug) / len(X_train):.1f}x\")\n",
    "\n",
    "# Convert augmented labels to categorical\n",
    "# For 3-class problems: multiply by 2 to convert 0.0, 0.5, 1.0 -> 0, 1, 2\n",
    "y_p_train_aug_cat = tf.keras.utils.to_categorical(y_p_train_aug * 2, num_classes=3)\n",
    "y_t_train_aug_cat = tf.keras.utils.to_categorical(y_t_train_aug * 2, num_classes=3)\n",
    "y_c_train_aug_cat = tf.keras.utils.to_categorical(y_c_train_aug * 2, num_classes=3)\n",
    "\n",
    "# For 2-class problems: convert 0.0, 1.0 -> 0, 1 (no multiplication needed)\n",
    "y_sp_train_aug_cat = tf.keras.utils.to_categorical(y_sp_train_aug, num_classes=2)\n",
    "\n",
    "print(\"Data augmentation completed for fine-tuning!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Build Model with Pre-trained Initialization\n",
    "\n",
    "**Key Change**: Model uses pre-trained weights as **initialization** (not frozen). All layers are trainable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CORRECTED: Build model with frozen pre-trained encoder (like original working version)\n",
    "# print(\"Building model with frozen pre-trained encoder...\")\n",
    "# model = build_frozen_encoder_model(\n",
    "#     input_shape=(60, 3),\n",
    "#     n_classes_p=3, \n",
    "#     n_classes_t=3, \n",
    "#     n_classes_c=3,\n",
    "#     pretrained_encoder=pretrained_encoder\n",
    "# )\n",
    "\n",
    "# print(f\"\\nModel parameters: {model.count_params():,}\")\n",
    "# print(\"Pre-trained encoder is frozen, new layers are trainable\")\n",
    "# model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Fine-tuning Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model with exact architecture match for successful weight copying...\n",
      "Attempting to copy weights from pre-trained encoder with exact architecture match...\n",
      "‚úì Copied weights for layer 0: sensor_input\n",
      "‚úì Copied weights for layer 1: conv1\n",
      "‚úì Copied weights for layer 2: bn1\n",
      "‚úì Copied weights for layer 3: dropout1\n",
      "‚úì Copied weights for layer 4: conv2\n",
      "‚úì Copied weights for layer 5: bn2\n",
      "‚úì Copied weights for layer 6: dropout2\n",
      "‚úì Copied weights for layer 7: conv3\n",
      "‚úì Copied weights for layer 8: bn3\n",
      "‚úì Copied weights for layer 9: dropout3\n",
      "‚úì Copied weights for layer 10: global_pool\n",
      "‚úì Copied weights for layer 11: dense1\n",
      "‚úì Copied weights for layer 12: dropout4\n",
      "‚úì Copied weights for layer 13: dense2\n",
      "‚úì Copied weights for layer 14: dropout5\n",
      "‚úì Copied weights for layer 15: concept_features\n",
      "‚úì Pre-trained weights copied successfully with exact architecture match!\n",
      "\n",
      "Model parameters: 27,904\n",
      "All layers are trainable (pre-trained weights copied successfully)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_7\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_7\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ<span style=\"font-weight: bold\"> Layer (type)        </span>‚îÉ<span style=\"font-weight: bold\"> Output Shape      </span>‚îÉ<span style=\"font-weight: bold\">    Param # </span>‚îÉ<span style=\"font-weight: bold\"> Connected to      </span>‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ sensor_input        ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ -                 ‚îÇ\n",
       "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)      ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    ‚îÇ      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> ‚îÇ sensor_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">‚Ä¶</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ bn1                 ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    ‚îÇ        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> ‚îÇ conv1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       ‚îÇ\n",
       "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio‚Ä¶</span> ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)  ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ bn1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)      ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    ‚îÇ     <span style=\"color: #00af00; text-decoration-color: #00af00\">10,272</span> ‚îÇ dropout1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ bn2                 ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    ‚îÇ        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> ‚îÇ conv2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       ‚îÇ\n",
       "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio‚Ä¶</span> ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)  ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ bn2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ conv3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)      ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)    ‚îÇ      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,576</span> ‚îÇ dropout2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ bn3                 ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)    ‚îÇ         <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> ‚îÇ conv3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       ‚îÇ\n",
       "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio‚Ä¶</span> ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)  ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)    ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ bn3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ global_pool         ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ dropout3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    ‚îÇ\n",
       "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool‚Ä¶</span> ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       ‚îÇ      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,176</span> ‚îÇ global_pool[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)  ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ dense1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        ‚îÇ      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> ‚îÇ dropout4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)  ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ dense2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ concept_features    ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)         ‚îÇ        <span style=\"color: #00af00; text-decoration-color: #00af00\">325</span> ‚îÇ dropout5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    ‚îÇ\n",
       "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ concept_dense_1     ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        ‚îÇ        <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span> ‚îÇ concept_features‚Ä¶ ‚îÇ\n",
       "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ concept_dropout_1   ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ concept_dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">‚Ä¶</span> ‚îÇ\n",
       "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ concept_dense_2     ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        ‚îÇ      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> ‚îÇ concept_dropout_‚Ä¶ ‚îÇ\n",
       "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ concept_dropout_2   ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ concept_dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">‚Ä¶</span> ‚îÇ\n",
       "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ periodicity (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>) ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         ‚îÇ         <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span> ‚îÇ concept_dropout_‚Ä¶ ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ temporal_stability  ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         ‚îÇ         <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span> ‚îÇ concept_dropout_‚Ä¶ ‚îÇ\n",
       "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ coordination        ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         ‚îÇ         <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span> ‚îÇ concept_dropout_‚Ä¶ ‚îÇ\n",
       "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ motion_intensity    ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         ‚îÇ         <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> ‚îÇ concept_dropout_‚Ä¶ ‚îÇ\n",
       "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ vertical_dominance  ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         ‚îÇ         <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> ‚îÇ concept_dropout_‚Ä¶ ‚îÇ\n",
       "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ sensor_input        ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m3\u001b[0m)     ‚îÇ          \u001b[38;5;34m0\u001b[0m ‚îÇ -                 ‚îÇ\n",
       "‚îÇ (\u001b[38;5;33mInputLayer\u001b[0m)        ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ conv1 (\u001b[38;5;33mConv1D\u001b[0m)      ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m64\u001b[0m)    ‚îÇ      \u001b[38;5;34m1,024\u001b[0m ‚îÇ sensor_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m‚Ä¶\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ bn1                 ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m64\u001b[0m)    ‚îÇ        \u001b[38;5;34m256\u001b[0m ‚îÇ conv1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       ‚îÇ\n",
       "‚îÇ (\u001b[38;5;33mBatchNormalizatio‚Ä¶\u001b[0m ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout1 (\u001b[38;5;33mDropout\u001b[0m)  ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m64\u001b[0m)    ‚îÇ          \u001b[38;5;34m0\u001b[0m ‚îÇ bn1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ conv2 (\u001b[38;5;33mConv1D\u001b[0m)      ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m32\u001b[0m)    ‚îÇ     \u001b[38;5;34m10,272\u001b[0m ‚îÇ dropout1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ bn2                 ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m32\u001b[0m)    ‚îÇ        \u001b[38;5;34m128\u001b[0m ‚îÇ conv2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       ‚îÇ\n",
       "‚îÇ (\u001b[38;5;33mBatchNormalizatio‚Ä¶\u001b[0m ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout2 (\u001b[38;5;33mDropout\u001b[0m)  ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m32\u001b[0m)    ‚îÇ          \u001b[38;5;34m0\u001b[0m ‚îÇ bn2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ conv3 (\u001b[38;5;33mConv1D\u001b[0m)      ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m16\u001b[0m)    ‚îÇ      \u001b[38;5;34m2,576\u001b[0m ‚îÇ dropout2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ bn3                 ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m16\u001b[0m)    ‚îÇ         \u001b[38;5;34m64\u001b[0m ‚îÇ conv3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       ‚îÇ\n",
       "‚îÇ (\u001b[38;5;33mBatchNormalizatio‚Ä¶\u001b[0m ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout3 (\u001b[38;5;33mDropout\u001b[0m)  ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m16\u001b[0m)    ‚îÇ          \u001b[38;5;34m0\u001b[0m ‚îÇ bn3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ global_pool         ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)        ‚îÇ          \u001b[38;5;34m0\u001b[0m ‚îÇ dropout3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    ‚îÇ\n",
       "‚îÇ (\u001b[38;5;33mGlobalAveragePool‚Ä¶\u001b[0m ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense1 (\u001b[38;5;33mDense\u001b[0m)      ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       ‚îÇ      \u001b[38;5;34m2,176\u001b[0m ‚îÇ global_pool[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout4 (\u001b[38;5;33mDropout\u001b[0m)  ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       ‚îÇ          \u001b[38;5;34m0\u001b[0m ‚îÇ dense1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense2 (\u001b[38;5;33mDense\u001b[0m)      ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        ‚îÇ      \u001b[38;5;34m8,256\u001b[0m ‚îÇ dropout4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout5 (\u001b[38;5;33mDropout\u001b[0m)  ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        ‚îÇ          \u001b[38;5;34m0\u001b[0m ‚îÇ dense2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ concept_features    ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)         ‚îÇ        \u001b[38;5;34m325\u001b[0m ‚îÇ dropout5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    ‚îÇ\n",
       "‚îÇ (\u001b[38;5;33mDense\u001b[0m)             ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ concept_dense_1     ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        ‚îÇ        \u001b[38;5;34m384\u001b[0m ‚îÇ concept_features‚Ä¶ ‚îÇ\n",
       "‚îÇ (\u001b[38;5;33mDense\u001b[0m)             ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ concept_dropout_1   ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        ‚îÇ          \u001b[38;5;34m0\u001b[0m ‚îÇ concept_dense_1[\u001b[38;5;34m‚Ä¶\u001b[0m ‚îÇ\n",
       "‚îÇ (\u001b[38;5;33mDropout\u001b[0m)           ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ concept_dense_2     ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        ‚îÇ      \u001b[38;5;34m2,080\u001b[0m ‚îÇ concept_dropout_‚Ä¶ ‚îÇ\n",
       "‚îÇ (\u001b[38;5;33mDense\u001b[0m)             ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ concept_dropout_2   ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        ‚îÇ          \u001b[38;5;34m0\u001b[0m ‚îÇ concept_dense_2[\u001b[38;5;34m‚Ä¶\u001b[0m ‚îÇ\n",
       "‚îÇ (\u001b[38;5;33mDropout\u001b[0m)           ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ periodicity (\u001b[38;5;33mDense\u001b[0m) ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)         ‚îÇ         \u001b[38;5;34m99\u001b[0m ‚îÇ concept_dropout_‚Ä¶ ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ temporal_stability  ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)         ‚îÇ         \u001b[38;5;34m99\u001b[0m ‚îÇ concept_dropout_‚Ä¶ ‚îÇ\n",
       "‚îÇ (\u001b[38;5;33mDense\u001b[0m)             ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ coordination        ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)         ‚îÇ         \u001b[38;5;34m99\u001b[0m ‚îÇ concept_dropout_‚Ä¶ ‚îÇ\n",
       "‚îÇ (\u001b[38;5;33mDense\u001b[0m)             ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ motion_intensity    ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         ‚îÇ         \u001b[38;5;34m33\u001b[0m ‚îÇ concept_dropout_‚Ä¶ ‚îÇ\n",
       "‚îÇ (\u001b[38;5;33mDense\u001b[0m)             ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ vertical_dominance  ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         ‚îÇ         \u001b[38;5;34m33\u001b[0m ‚îÇ concept_dropout_‚Ä¶ ‚îÇ\n",
       "‚îÇ (\u001b[38;5;33mDense\u001b[0m)             ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">27,904</span> (109.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m27,904\u001b[0m (109.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">27,680</span> (108.12 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m27,680\u001b[0m (108.12 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">224</span> (896.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m224\u001b[0m (896.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build model with EXACT architecture match for successful weight copying\n",
    "print(\"Building model with exact architecture match for successful weight copying...\")\n",
    "model = build_exact_match_model_with_pretrained_encoder(\n",
    "    input_shape=(60, 3),\n",
    "    n_classes_p=3, \n",
    "    n_classes_t=3, \n",
    "    n_classes_c=3,\n",
    "    pretrained_encoder=pretrained_encoder\n",
    ")\n",
    "\n",
    "print(f\"\\nModel parameters: {model.count_params():,}\")\n",
    "print(\"All layers are trainable (pre-trained weights copied successfully)\")\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling model with weighted losses to prioritize regression tasks...\n",
      "Fine-tuning model compiled successfully!\n",
      "Using 5x higher loss weights for regression tasks to balance with classification tasks\n",
      "Training data prepared for fine-tuning!\n",
      "Starting fine-tuning training with weighted losses...\n",
      "Regression tasks have 5x higher loss weights to balance with classification\n",
      "Epoch 1/50\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - coordination_accuracy: 0.4750 - coordination_loss: 0.9977 - loss: 3.8087 - motion_intensity_loss: 0.0591 - motion_intensity_mae: 0.1776 - periodicity_accuracy: 0.4058 - periodicity_loss: 1.0572 - temporal_stability_accuracy: 0.3325 - temporal_stability_loss: 1.0575 - vertical_dominance_loss: 0.0791 - vertical_dominance_mae: 0.2215 - val_coordination_accuracy: 0.4333 - val_coordination_loss: 0.9994 - val_loss: 4.5277 - val_motion_intensity_loss: 0.2334 - val_motion_intensity_mae: 0.4315 - val_periodicity_accuracy: 0.3333 - val_periodicity_loss: 1.0570 - val_temporal_stability_accuracy: 0.4000 - val_temporal_stability_loss: 1.0404 - val_vertical_dominance_loss: 0.0528 - val_vertical_dominance_mae: 0.1963 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - coordination_accuracy: 0.4933 - coordination_loss: 0.9369 - loss: 3.2632 - motion_intensity_loss: 0.0353 - motion_intensity_mae: 0.1361 - periodicity_accuracy: 0.5992 - periodicity_loss: 0.9611 - temporal_stability_accuracy: 0.5017 - temporal_stability_loss: 0.9729 - vertical_dominance_loss: 0.0432 - vertical_dominance_mae: 0.1558 - val_coordination_accuracy: 0.4333 - val_coordination_loss: 0.9197 - val_loss: 3.3726 - val_motion_intensity_loss: 0.0862 - val_motion_intensity_mae: 0.2606 - val_periodicity_accuracy: 0.6000 - val_periodicity_loss: 0.9721 - val_temporal_stability_accuracy: 0.3333 - val_temporal_stability_loss: 0.9682 - val_vertical_dominance_loss: 0.0163 - val_vertical_dominance_mae: 0.0939 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - coordination_accuracy: 0.5475 - coordination_loss: 0.8543 - loss: 2.9514 - motion_intensity_loss: 0.0297 - motion_intensity_mae: 0.1281 - periodicity_accuracy: 0.6042 - periodicity_loss: 0.8507 - temporal_stability_accuracy: 0.6900 - temporal_stability_loss: 0.8616 - vertical_dominance_loss: 0.0465 - vertical_dominance_mae: 0.1586 - val_coordination_accuracy: 0.7000 - val_coordination_loss: 0.8755 - val_loss: 2.9983 - val_motion_intensity_loss: 0.0445 - val_motion_intensity_mae: 0.1853 - val_periodicity_accuracy: 0.6000 - val_periodicity_loss: 0.9392 - val_temporal_stability_accuracy: 0.6333 - val_temporal_stability_loss: 0.8785 - val_vertical_dominance_loss: 0.0165 - val_vertical_dominance_mae: 0.0958 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - coordination_accuracy: 0.6442 - coordination_loss: 0.7872 - loss: 2.6976 - motion_intensity_loss: 0.0295 - motion_intensity_mae: 0.1294 - periodicity_accuracy: 0.6125 - periodicity_loss: 0.7993 - temporal_stability_accuracy: 0.7967 - temporal_stability_loss: 0.7523 - vertical_dominance_loss: 0.0422 - vertical_dominance_mae: 0.1517 - val_coordination_accuracy: 0.7000 - val_coordination_loss: 0.7902 - val_loss: 2.6798 - val_motion_intensity_loss: 0.0326 - val_motion_intensity_mae: 0.1541 - val_periodicity_accuracy: 0.6000 - val_periodicity_loss: 0.8764 - val_temporal_stability_accuracy: 0.7333 - val_temporal_stability_loss: 0.7734 - val_vertical_dominance_loss: 0.0154 - val_vertical_dominance_mae: 0.0907 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - coordination_accuracy: 0.7192 - coordination_loss: 0.7371 - loss: 2.5001 - motion_intensity_loss: 0.0245 - motion_intensity_mae: 0.1185 - periodicity_accuracy: 0.6242 - periodicity_loss: 0.7811 - temporal_stability_accuracy: 0.8117 - temporal_stability_loss: 0.6731 - vertical_dominance_loss: 0.0363 - vertical_dominance_mae: 0.1417 - val_coordination_accuracy: 0.8000 - val_coordination_loss: 0.7117 - val_loss: 2.4634 - val_motion_intensity_loss: 0.0263 - val_motion_intensity_mae: 0.1372 - val_periodicity_accuracy: 0.6667 - val_periodicity_loss: 0.8196 - val_temporal_stability_accuracy: 0.7333 - val_temporal_stability_loss: 0.7364 - val_vertical_dominance_loss: 0.0128 - val_vertical_dominance_mae: 0.0840 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - coordination_accuracy: 0.7275 - coordination_loss: 0.6956 - loss: 2.3978 - motion_intensity_loss: 0.0285 - motion_intensity_mae: 0.1260 - periodicity_accuracy: 0.6208 - periodicity_loss: 0.7589 - temporal_stability_accuracy: 0.8200 - temporal_stability_loss: 0.6168 - vertical_dominance_loss: 0.0364 - vertical_dominance_mae: 0.1414 - val_coordination_accuracy: 0.8000 - val_coordination_loss: 0.6680 - val_loss: 2.3572 - val_motion_intensity_loss: 0.0213 - val_motion_intensity_mae: 0.0966 - val_periodicity_accuracy: 0.6000 - val_periodicity_loss: 0.7683 - val_temporal_stability_accuracy: 0.7667 - val_temporal_stability_loss: 0.7347 - val_vertical_dominance_loss: 0.0159 - val_vertical_dominance_mae: 0.0990 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - coordination_accuracy: 0.7308 - coordination_loss: 0.6818 - loss: 2.2996 - motion_intensity_loss: 0.0229 - motion_intensity_mae: 0.1108 - periodicity_accuracy: 0.6208 - periodicity_loss: 0.7439 - temporal_stability_accuracy: 0.8175 - temporal_stability_loss: 0.5895 - vertical_dominance_loss: 0.0334 - vertical_dominance_mae: 0.1375 - val_coordination_accuracy: 0.8000 - val_coordination_loss: 0.6222 - val_loss: 2.2554 - val_motion_intensity_loss: 0.0222 - val_motion_intensity_mae: 0.1075 - val_periodicity_accuracy: 0.6000 - val_periodicity_loss: 0.7472 - val_temporal_stability_accuracy: 0.7667 - val_temporal_stability_loss: 0.7006 - val_vertical_dominance_loss: 0.0149 - val_vertical_dominance_mae: 0.0954 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - coordination_accuracy: 0.7317 - coordination_loss: 0.6816 - loss: 2.2366 - motion_intensity_loss: 0.0207 - motion_intensity_mae: 0.1070 - periodicity_accuracy: 0.6342 - periodicity_loss: 0.7347 - temporal_stability_accuracy: 0.8200 - temporal_stability_loss: 0.5626 - vertical_dominance_loss: 0.0308 - vertical_dominance_mae: 0.1327 - val_coordination_accuracy: 0.8000 - val_coordination_loss: 0.6371 - val_loss: 2.2772 - val_motion_intensity_loss: 0.0193 - val_motion_intensity_mae: 0.0986 - val_periodicity_accuracy: 0.6667 - val_periodicity_loss: 0.7472 - val_temporal_stability_accuracy: 0.7333 - val_temporal_stability_loss: 0.7184 - val_vertical_dominance_loss: 0.0156 - val_vertical_dominance_mae: 0.0958 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - coordination_accuracy: 0.7458 - coordination_loss: 0.6500 - loss: 2.1858 - motion_intensity_loss: 0.0215 - motion_intensity_mae: 0.1045 - periodicity_accuracy: 0.6525 - periodicity_loss: 0.7239 - temporal_stability_accuracy: 0.8200 - temporal_stability_loss: 0.5380 - vertical_dominance_loss: 0.0323 - vertical_dominance_mae: 0.1323 - val_coordination_accuracy: 0.8000 - val_coordination_loss: 0.5981 - val_loss: 2.1948 - val_motion_intensity_loss: 0.0231 - val_motion_intensity_mae: 0.1103 - val_periodicity_accuracy: 0.5667 - val_periodicity_loss: 0.7771 - val_temporal_stability_accuracy: 0.7333 - val_temporal_stability_loss: 0.6224 - val_vertical_dominance_loss: 0.0164 - val_vertical_dominance_mae: 0.0993 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - coordination_accuracy: 0.7417 - coordination_loss: 0.6398 - loss: 2.1207 - motion_intensity_loss: 0.0221 - motion_intensity_mae: 0.1055 - periodicity_accuracy: 0.6417 - periodicity_loss: 0.7195 - temporal_stability_accuracy: 0.8225 - temporal_stability_loss: 0.5109 - vertical_dominance_loss: 0.0279 - vertical_dominance_mae: 0.1271 - val_coordination_accuracy: 0.8000 - val_coordination_loss: 0.5640 - val_loss: 2.1821 - val_motion_intensity_loss: 0.0246 - val_motion_intensity_mae: 0.0984 - val_periodicity_accuracy: 0.6000 - val_periodicity_loss: 0.7968 - val_temporal_stability_accuracy: 0.7000 - val_temporal_stability_loss: 0.6187 - val_vertical_dominance_loss: 0.0159 - val_vertical_dominance_mae: 0.0943 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - coordination_accuracy: 0.7450 - coordination_loss: 0.6321 - loss: 2.0570 - motion_intensity_loss: 0.0192 - motion_intensity_mae: 0.1009 - periodicity_accuracy: 0.6392 - periodicity_loss: 0.7237 - temporal_stability_accuracy: 0.8175 - temporal_stability_loss: 0.4838 - vertical_dominance_loss: 0.0254 - vertical_dominance_mae: 0.1200 - val_coordination_accuracy: 0.8333 - val_coordination_loss: 0.5481 - val_loss: 2.0914 - val_motion_intensity_loss: 0.0227 - val_motion_intensity_mae: 0.1046 - val_periodicity_accuracy: 0.5333 - val_periodicity_loss: 0.8005 - val_temporal_stability_accuracy: 0.7333 - val_temporal_stability_loss: 0.5508 - val_vertical_dominance_loss: 0.0157 - val_vertical_dominance_mae: 0.0942 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - coordination_accuracy: 0.7533 - coordination_loss: 0.6248 - loss: 2.0120 - motion_intensity_loss: 0.0183 - motion_intensity_mae: 0.0965 - periodicity_accuracy: 0.6417 - periodicity_loss: 0.7078 - temporal_stability_accuracy: 0.8275 - temporal_stability_loss: 0.4535 - vertical_dominance_loss: 0.0266 - vertical_dominance_mae: 0.1219 - val_coordination_accuracy: 0.8000 - val_coordination_loss: 0.6009 - val_loss: 2.1120 - val_motion_intensity_loss: 0.0185 - val_motion_intensity_mae: 0.0873 - val_periodicity_accuracy: 0.7000 - val_periodicity_loss: 0.7559 - val_temporal_stability_accuracy: 0.7667 - val_temporal_stability_loss: 0.5856 - val_vertical_dominance_loss: 0.0154 - val_vertical_dominance_mae: 0.0991 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - coordination_accuracy: 0.7558 - coordination_loss: 0.6001 - loss: 1.9344 - motion_intensity_loss: 0.0159 - motion_intensity_mae: 0.0910 - periodicity_accuracy: 0.6808 - periodicity_loss: 0.6825 - temporal_stability_accuracy: 0.8392 - temporal_stability_loss: 0.4375 - vertical_dominance_loss: 0.0257 - vertical_dominance_mae: 0.1183 - val_coordination_accuracy: 0.8000 - val_coordination_loss: 0.5165 - val_loss: 2.0750 - val_motion_intensity_loss: 0.0216 - val_motion_intensity_mae: 0.0950 - val_periodicity_accuracy: 0.6000 - val_periodicity_loss: 0.7759 - val_temporal_stability_accuracy: 0.8000 - val_temporal_stability_loss: 0.5860 - val_vertical_dominance_loss: 0.0177 - val_vertical_dominance_mae: 0.1058 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - coordination_accuracy: 0.7642 - coordination_loss: 0.5909 - loss: 1.9007 - motion_intensity_loss: 0.0185 - motion_intensity_mae: 0.0970 - periodicity_accuracy: 0.6717 - periodicity_loss: 0.6770 - temporal_stability_accuracy: 0.8425 - temporal_stability_loss: 0.4239 - vertical_dominance_loss: 0.0256 - vertical_dominance_mae: 0.1196 - val_coordination_accuracy: 0.8000 - val_coordination_loss: 0.4887 - val_loss: 1.8780 - val_motion_intensity_loss: 0.0175 - val_motion_intensity_mae: 0.0977 - val_periodicity_accuracy: 0.5667 - val_periodicity_loss: 0.7071 - val_temporal_stability_accuracy: 0.7667 - val_temporal_stability_loss: 0.5207 - val_vertical_dominance_loss: 0.0148 - val_vertical_dominance_mae: 0.0888 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - coordination_accuracy: 0.7700 - coordination_loss: 0.5693 - loss: 1.8804 - motion_intensity_loss: 0.0204 - motion_intensity_mae: 0.1019 - periodicity_accuracy: 0.6567 - periodicity_loss: 0.6692 - temporal_stability_accuracy: 0.8367 - temporal_stability_loss: 0.4178 - vertical_dominance_loss: 0.0248 - vertical_dominance_mae: 0.1178 - val_coordination_accuracy: 0.8000 - val_coordination_loss: 0.5349 - val_loss: 2.0191 - val_motion_intensity_loss: 0.0208 - val_motion_intensity_mae: 0.0871 - val_periodicity_accuracy: 0.6333 - val_periodicity_loss: 0.7139 - val_temporal_stability_accuracy: 0.7333 - val_temporal_stability_loss: 0.5798 - val_vertical_dominance_loss: 0.0173 - val_vertical_dominance_mae: 0.1035 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - coordination_accuracy: 0.7792 - coordination_loss: 0.5503 - loss: 1.8214 - motion_intensity_loss: 0.0191 - motion_intensity_mae: 0.0972 - periodicity_accuracy: 0.6942 - periodicity_loss: 0.6317 - temporal_stability_accuracy: 0.8400 - temporal_stability_loss: 0.4308 - vertical_dominance_loss: 0.0241 - vertical_dominance_mae: 0.1156 - val_coordination_accuracy: 0.8333 - val_coordination_loss: 0.5014 - val_loss: 1.9809 - val_motion_intensity_loss: 0.0182 - val_motion_intensity_mae: 0.0912 - val_periodicity_accuracy: 0.7000 - val_periodicity_loss: 0.7014 - val_temporal_stability_accuracy: 0.7667 - val_temporal_stability_loss: 0.6077 - val_vertical_dominance_loss: 0.0159 - val_vertical_dominance_mae: 0.0947 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - coordination_accuracy: 0.7783 - coordination_loss: 0.5348 - loss: 1.7301 - motion_intensity_loss: 0.0198 - motion_intensity_mae: 0.1004 - periodicity_accuracy: 0.7233 - periodicity_loss: 0.5864 - temporal_stability_accuracy: 0.8508 - temporal_stability_loss: 0.3853 - vertical_dominance_loss: 0.0238 - vertical_dominance_mae: 0.1165 - val_coordination_accuracy: 0.8000 - val_coordination_loss: 0.5069 - val_loss: 2.1257 - val_motion_intensity_loss: 0.0235 - val_motion_intensity_mae: 0.1054 - val_periodicity_accuracy: 0.7000 - val_periodicity_loss: 0.7725 - val_temporal_stability_accuracy: 0.7333 - val_temporal_stability_loss: 0.6342 - val_vertical_dominance_loss: 0.0189 - val_vertical_dominance_mae: 0.1065 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - coordination_accuracy: 0.7842 - coordination_loss: 0.5172 - loss: 1.6468 - motion_intensity_loss: 0.0230 - motion_intensity_mae: 0.1025 - periodicity_accuracy: 0.7233 - periodicity_loss: 0.5476 - temporal_stability_accuracy: 0.8517 - temporal_stability_loss: 0.3614 - vertical_dominance_loss: 0.0220 - vertical_dominance_mae: 0.1102 - val_coordination_accuracy: 0.8333 - val_coordination_loss: 0.5048 - val_loss: 2.1119 - val_motion_intensity_loss: 0.0225 - val_motion_intensity_mae: 0.0949 - val_periodicity_accuracy: 0.6667 - val_periodicity_loss: 0.7753 - val_temporal_stability_accuracy: 0.7667 - val_temporal_stability_loss: 0.6331 - val_vertical_dominance_loss: 0.0172 - val_vertical_dominance_mae: 0.1034 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - coordination_accuracy: 0.7925 - coordination_loss: 0.4737 - loss: 1.5574 - motion_intensity_loss: 0.0215 - motion_intensity_mae: 0.1013 - periodicity_accuracy: 0.7625 - periodicity_loss: 0.4981 - temporal_stability_accuracy: 0.8492 - temporal_stability_loss: 0.3684 - vertical_dominance_loss: 0.0234 - vertical_dominance_mae: 0.1137 - val_coordination_accuracy: 0.7667 - val_coordination_loss: 0.4997 - val_loss: 1.8949 - val_motion_intensity_loss: 0.0239 - val_motion_intensity_mae: 0.1029 - val_periodicity_accuracy: 0.7667 - val_periodicity_loss: 0.6307 - val_temporal_stability_accuracy: 0.7667 - val_temporal_stability_loss: 0.5442 - val_vertical_dominance_loss: 0.0202 - val_vertical_dominance_mae: 0.1098 - learning_rate: 5.0000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - coordination_accuracy: 0.8108 - coordination_loss: 0.4519 - loss: 1.4784 - motion_intensity_loss: 0.0204 - motion_intensity_mae: 0.0990 - periodicity_accuracy: 0.7783 - periodicity_loss: 0.4705 - temporal_stability_accuracy: 0.8467 - temporal_stability_loss: 0.3392 - vertical_dominance_loss: 0.0230 - vertical_dominance_mae: 0.1126 - val_coordination_accuracy: 0.8000 - val_coordination_loss: 0.4925 - val_loss: 1.9869 - val_motion_intensity_loss: 0.0206 - val_motion_intensity_mae: 0.0957 - val_periodicity_accuracy: 0.7000 - val_periodicity_loss: 0.7362 - val_temporal_stability_accuracy: 0.7667 - val_temporal_stability_loss: 0.5669 - val_vertical_dominance_loss: 0.0177 - val_vertical_dominance_mae: 0.1058 - learning_rate: 5.0000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - coordination_accuracy: 0.8058 - coordination_loss: 0.4632 - loss: 1.5028 - motion_intensity_loss: 0.0212 - motion_intensity_mae: 0.0997 - periodicity_accuracy: 0.7642 - periodicity_loss: 0.4810 - temporal_stability_accuracy: 0.8583 - temporal_stability_loss: 0.3426 - vertical_dominance_loss: 0.0234 - vertical_dominance_mae: 0.1130 - val_coordination_accuracy: 0.8333 - val_coordination_loss: 0.5149 - val_loss: 2.0897 - val_motion_intensity_loss: 0.0240 - val_motion_intensity_mae: 0.1001 - val_periodicity_accuracy: 0.6667 - val_periodicity_loss: 0.7504 - val_temporal_stability_accuracy: 0.7667 - val_temporal_stability_loss: 0.6069 - val_vertical_dominance_loss: 0.0195 - val_vertical_dominance_mae: 0.1076 - learning_rate: 5.0000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - coordination_accuracy: 0.8175 - coordination_loss: 0.4243 - loss: 1.3947 - motion_intensity_loss: 0.0183 - motion_intensity_mae: 0.0943 - periodicity_accuracy: 0.8025 - periodicity_loss: 0.4447 - temporal_stability_accuracy: 0.8575 - temporal_stability_loss: 0.3344 - vertical_dominance_loss: 0.0212 - vertical_dominance_mae: 0.1081 - val_coordination_accuracy: 0.8000 - val_coordination_loss: 0.5362 - val_loss: 2.0980 - val_motion_intensity_loss: 0.0228 - val_motion_intensity_mae: 0.0978 - val_periodicity_accuracy: 0.7000 - val_periodicity_loss: 0.7307 - val_temporal_stability_accuracy: 0.7667 - val_temporal_stability_loss: 0.6289 - val_vertical_dominance_loss: 0.0176 - val_vertical_dominance_mae: 0.1053 - learning_rate: 5.0000e-04\n",
      "Fine-tuning training completed!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Compile the model with WEIGHTED LOSSES for better regression performance\n",
    "print(\"Compiling model with weighted losses to prioritize regression tasks...\")\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),  # Original learning rate\n",
    "    loss={\n",
    "        'periodicity': 'categorical_crossentropy',\n",
    "        'temporal_stability': 'categorical_crossentropy', \n",
    "        'coordination': 'categorical_crossentropy',\n",
    "        'motion_intensity': 'mse',  # Regression loss\n",
    "        'vertical_dominance': 'mse'  # Regression loss\n",
    "    },\n",
    "    loss_weights={\n",
    "        'periodicity': 1.0,           # Classification tasks\n",
    "        'temporal_stability': 1.0,    # Classification tasks\n",
    "        'coordination': 1.0,          # Classification tasks\n",
    "        'motion_intensity': 5.0,      # Higher weight for regression\n",
    "        'vertical_dominance': 5.0     # Higher weight for regression\n",
    "    },\n",
    "    metrics={\n",
    "        'periodicity': ['accuracy'],\n",
    "        'temporal_stability': ['accuracy'],\n",
    "        'coordination': ['accuracy'],\n",
    "        'motion_intensity': ['mae'],  # Regression metric\n",
    "        'vertical_dominance': ['mae']  # Regression metric\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Fine-tuning model compiled successfully!\")\n",
    "print(\"Using 5x higher loss weights for regression tasks to balance with classification tasks\")\n",
    "\n",
    "# Keep continuous concepts as regression (no categorical conversion)\n",
    "# Only convert discrete concepts to categorical\n",
    "y_p_train_aug_cat = tf.keras.utils.to_categorical(y_p_train_aug * 2, num_classes=3)\n",
    "y_t_train_aug_cat = tf.keras.utils.to_categorical(y_t_train_aug * 2, num_classes=3)\n",
    "y_c_train_aug_cat = tf.keras.utils.to_categorical(y_c_train_aug * 2, num_classes=3)\n",
    "\n",
    "y_p_test_cat = tf.keras.utils.to_categorical(y_p_test * 2, num_classes=3)\n",
    "y_t_test_cat = tf.keras.utils.to_categorical(y_t_test * 2, num_classes=3)\n",
    "y_c_test_cat = tf.keras.utils.to_categorical(y_c_test * 2, num_classes=3)\n",
    "\n",
    "# Prepare training data (3 discrete + 2 continuous)\n",
    "train_targets = {\n",
    "    'periodicity': y_p_train_aug_cat,\n",
    "    'temporal_stability': y_t_train_aug_cat,\n",
    "    'coordination': y_c_train_aug_cat,\n",
    "    'motion_intensity': y_mi_train_aug,  # Keep as continuous\n",
    "    'vertical_dominance': y_vd_train_aug  # Keep as continuous\n",
    "}\n",
    "\n",
    "# Prepare validation data\n",
    "val_targets = {\n",
    "    'periodicity': y_p_test_cat,\n",
    "    'temporal_stability': y_t_test_cat,\n",
    "    'coordination': y_c_test_cat,\n",
    "    'motion_intensity': y_mi_test,  # Keep as continuous\n",
    "    'vertical_dominance': y_vd_test  # Keep as continuous\n",
    "}\n",
    "\n",
    "print(\"Training data prepared for fine-tuning!\")\n",
    "\n",
    "# Train the fine-tuning model with weighted losses\n",
    "print(\"Starting fine-tuning training with weighted losses...\")\n",
    "print(\"Regression tasks have 5x higher loss weights to balance with classification\")\n",
    "history = model.fit(\n",
    "    X_train_aug, train_targets,\n",
    "    validation_data=(X_test, val_targets),\n",
    "    epochs=50,  # Fewer epochs for fine-tuning\n",
    "    batch_size=32,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(patience=8, restore_best_weights=True),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=4)\n",
    "    ],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Fine-tuning training completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation with AUROC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ calculate_auroc_finetuning function defined!\n"
     ]
    }
   ],
   "source": [
    "# Missing function: calculate_auroc_finetuning\n",
    "def calculate_auroc_finetuning(y_true, y_pred, concept_name, n_classes):\n",
    "    \"\"\"\n",
    "    Calculate AUROC for multi-class classification in fine-tuning context.\n",
    "    \n",
    "    Args:\n",
    "        y_true: True labels (one-hot encoded or class indices)\n",
    "        y_pred: Predicted probabilities (shape: [n_samples, n_classes])\n",
    "        concept_name: Name of the concept for logging\n",
    "        n_classes: Number of classes\n",
    "    \n",
    "    Returns:\n",
    "        AUROC score (float)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        import numpy as np\n",
    "        \n",
    "        # Handle one-hot encoded labels\n",
    "        if len(y_true.shape) > 1 and y_true.shape[1] > 1:\n",
    "            # Convert one-hot to class indices\n",
    "            y_true_classes = np.argmax(y_true, axis=1)\n",
    "        else:\n",
    "            y_true_classes = y_true.flatten()\n",
    "        \n",
    "        # For multi-class AUROC, we need to use the 'ovr' (one-vs-rest) strategy\n",
    "        if n_classes > 2:\n",
    "            # Multi-class AUROC using one-vs-rest\n",
    "            auroc = roc_auc_score(y_true_classes, y_pred, multi_class='ovr', average='macro')\n",
    "        else:\n",
    "            # Binary classification\n",
    "            auroc = roc_auc_score(y_true_classes, y_pred[:, 1])\n",
    "        \n",
    "        print(f\"‚úì {concept_name} AUROC: {auroc:.4f}\")\n",
    "        return auroc\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö† Error calculating AUROC for {concept_name}: {e}\")\n",
    "        return 0.5  # Return neutral score if calculation fails\n",
    "\n",
    "print(\"‚úÖ calculate_auroc_finetuning function defined!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model with scaled regression targets and weighted losses...\n",
      "‚úì periodicity AUROC: 0.8038\n",
      "‚úì temporal_stability AUROC: 0.9349\n",
      "‚úì coordination AUROC: 0.9165\n",
      "\n",
      "=== WEIGHTED LOSS MODEL RESULTS (3 DISCRETE + 2 CONTINUOUS) ===\n",
      "\n",
      "--- Discrete Concepts (Classification) ---\n",
      "Periodicity - Accuracy: 0.5667, AUROC: 0.8038\n",
      "Temporal Stability - Accuracy: 0.7667, AUROC: 0.9349\n",
      "Coordination - Accuracy: 0.8000, AUROC: 0.9165\n",
      "\n",
      "--- Continuous Concepts (Regression) ---\n",
      "Motion Intensity - R¬≤ (scaled): 0.6491, R¬≤ (original): 0.2361\n",
      "Vertical Dominance - R¬≤ (scaled): 0.1726, R¬≤ (original): -0.5483\n",
      "\n",
      "--- Overall Performance ---\n",
      "Overall Average Accuracy (discrete): 71.1%\n",
      "Overall Average R¬≤ (continuous, original scale): -0.1561\n",
      "Overall Average AUROC (discrete): 0.8851\n",
      "\n",
      "Model saved as 'weighted_cnn_with_pretrained_encoder.keras'\n",
      "Evaluation completed!\n"
     ]
    }
   ],
   "source": [
    "# CORRECTED: Evaluation with Mixed Data Types (3 discrete + 2 continuous) - SCALED REGRESSION + WEIGHTED LOSSES\n",
    "print(\"Evaluating model with scaled regression targets and weighted losses...\")\n",
    "results = model.evaluate(X_test, val_targets, verbose=0)\n",
    "\n",
    "# Get predictions\n",
    "predictions = model.predict(X_test, verbose=0)\n",
    "\n",
    "# Discrete concepts: use argmax for classification\n",
    "periodicity_pred = np.argmax(predictions[0], axis=1)\n",
    "temporal_stability_pred = np.argmax(predictions[1], axis=1)\n",
    "coordination_pred = np.argmax(predictions[2], axis=1)\n",
    "\n",
    "# Continuous concepts: use raw values for regression (these are now scaled 0-1)\n",
    "motion_intensity_pred_scaled = predictions[3].flatten()\n",
    "vertical_dominance_pred_scaled = predictions[4].flatten()\n",
    "\n",
    "# Calculate metrics for discrete concepts\n",
    "periodicity_acc = accuracy_score(np.argmax(val_targets['periodicity'], axis=1), periodicity_pred)\n",
    "temporal_stability_acc = accuracy_score(np.argmax(val_targets['temporal_stability'], axis=1), temporal_stability_pred)\n",
    "coordination_acc = accuracy_score(np.argmax(val_targets['coordination'], axis=1), coordination_pred)\n",
    "\n",
    "# Calculate R¬≤ for continuous concepts (using scaled targets and predictions)\n",
    "motion_intensity_r2_scaled = r2_score(val_targets['motion_intensity'], motion_intensity_pred_scaled)\n",
    "vertical_dominance_r2_scaled = r2_score(val_targets['vertical_dominance'], vertical_dominance_pred_scaled)\n",
    "\n",
    "# Inverse scale predictions to original range for comparison\n",
    "motion_intensity_pred_original = motion_intensity_pred_scaled * (mi_max - mi_min) + mi_min\n",
    "vertical_dominance_pred_original = vertical_dominance_pred_scaled * (vd_max - vd_min) + vd_min\n",
    "\n",
    "# Calculate R¬≤ on original scale for fair comparison\n",
    "motion_intensity_r2_original = r2_score(y_mi_test_original, motion_intensity_pred_original)\n",
    "vertical_dominance_r2_original = r2_score(y_vd_test_original, vertical_dominance_pred_original)\n",
    "\n",
    "# Calculate AUROC for discrete concepts only\n",
    "periodicity_auroc = calculate_auroc_finetuning(val_targets['periodicity'], predictions[0], 'periodicity', 3)\n",
    "temporal_stability_auroc = calculate_auroc_finetuning(val_targets['temporal_stability'], predictions[1], 'temporal_stability', 3)\n",
    "coordination_auroc = calculate_auroc_finetuning(val_targets['coordination'], predictions[2], 'coordination', 3)\n",
    "\n",
    "# Calculate overall metrics\n",
    "overall_acc = (periodicity_acc + temporal_stability_acc + coordination_acc) / 3  # Only discrete concepts\n",
    "auroc_scores = [periodicity_auroc, temporal_stability_auroc, coordination_auroc]\n",
    "valid_auroc_scores = [score for score in auroc_scores if not np.isnan(score)]\n",
    "overall_auroc = np.mean(valid_auroc_scores) if valid_auroc_scores else 0.5\n",
    "\n",
    "print(f\"\\n=== WEIGHTED LOSS MODEL RESULTS (3 DISCRETE + 2 CONTINUOUS) ===\")\n",
    "print(f\"\\n--- Discrete Concepts (Classification) ---\")\n",
    "print(f\"Periodicity - Accuracy: {periodicity_acc:.4f}, AUROC: {periodicity_auroc:.4f}\")\n",
    "print(f\"Temporal Stability - Accuracy: {temporal_stability_acc:.4f}, AUROC: {temporal_stability_auroc:.4f}\")\n",
    "print(f\"Coordination - Accuracy: {coordination_acc:.4f}, AUROC: {coordination_auroc:.4f}\")\n",
    "\n",
    "print(f\"\\n--- Continuous Concepts (Regression) ---\")\n",
    "print(f\"Motion Intensity - R¬≤ (scaled): {motion_intensity_r2_scaled:.4f}, R¬≤ (original): {motion_intensity_r2_original:.4f}\")\n",
    "print(f\"Vertical Dominance - R¬≤ (scaled): {vertical_dominance_r2_scaled:.4f}, R¬≤ (original): {vertical_dominance_r2_original:.4f}\")\n",
    "\n",
    "print(f\"\\n--- Overall Performance ---\")\n",
    "print(f\"Overall Average Accuracy (discrete): {overall_acc*100:.1f}%\")\n",
    "print(f\"Overall Average R¬≤ (continuous, original scale): {(motion_intensity_r2_original + vertical_dominance_r2_original) / 2:.4f}\")\n",
    "print(f\"Overall Average AUROC (discrete): {overall_auroc:.4f}\")\n",
    "\n",
    "# Save model\n",
    "model.save(\"weighted_cnn_with_pretrained_encoder.keras\")\n",
    "print(f\"\\nModel saved as 'weighted_cnn_with_pretrained_encoder.keras'\")\n",
    "\n",
    "print(\"Evaluation completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
